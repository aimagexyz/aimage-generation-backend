import asyncio
import io
import json
import os
import tempfile
import time
import traceback
import uuid
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Tuple
from uuid import UUID

import boto3
import httpx
import pandas as pd
from google import genai
from google.genai import types
from openpyxl import load_workbook
from openpyxl.cell.rich_text import CellRichText, TextBlock
from openpyxl.cell.text import InlineFont
from openpyxl.styles import Font
from PIL import Image
from pydantic import BaseModel, Field
from tortoise.exceptions import DoesNotExist
from tortoise.expressions import Q
from tortoise.transactions import in_transaction

from aimage_supervision.clients.aws_s3 import (
    download_file_content_from_s3_sync, download_image_from_s3,
    download_video_from_s3)
from aimage_supervision.endpoints.bounding_box_detect import \
    detect_bounding_boxes_new_sync
from aimage_supervision.enums import (AiReviewMode, AiReviewProcessingStatus,
                                      SubtaskType)
from aimage_supervision.models import (AiReview, AiReviewFindingEntry,
                                       Character, ReviewPointDefinition,
                                       ReviewPointDefinitionVersion, Subtask,
                                       User)
from aimage_supervision.prompts.review_point_prompts import (
    copyright_review_prompt, get_speaker_prompt, get_target_prompt,
    ng_review_prompt, visual_review_prompt)
from aimage_supervision.schemas import AiDetectedElement, AiDetectedElements
from aimage_supervision.schemas import AiReview as AiReviewSchema
from aimage_supervision.schemas import \
    AiReviewFindingEntry as AiReviewFindingEntrySchema
from aimage_supervision.schemas import (AiReviewFindingEntryInDB, AiReviewInDB,
                                        ReviewPointDefinitionVersionInDB)
from aimage_supervision.services.ai_review_pipeline import (
    BoundingBoxes, _generate_findings_for_copyright_review_sync,
    _generate_findings_for_ng_review_sync,
    _generate_findings_for_visual_review_sync,
    convert_bounding_boxes_to_x_y_width_height)
from aimage_supervision.services.ai_review_pipeline_multi_agent import \
    generate_findings_for_visual_review_multi_agent_sync
from aimage_supervision.services.rpd_create_service import split_review_prompt
from aimage_supervision.services.rpd_filter import filter_rpd_async
from aimage_supervision.settings import MAX_CONCURRENT, logger
from aimage_supervision.utils.video_utils import (
    detect_video_scenes_and_extract_frames, process_image_or_video_bytes)

# Model constants
QUALITY_MODEL = "gemini-2.5-pro"
SPEED_MODEL = "gemini-2.5-flash"

# Configure file handler for debugging in threads


if os.getenv('GEMINI_API_KEY'):
    gemini_client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))
else:
    gemini_client = None
    logger.warning(
        "GEMINI_API_KEY not set. detect_elements will not function.")


async def get_active_review_point_versions(
    project_id: Optional[UUID] = None,
    cr_check: Optional[bool] = False,
    rpd_ids: Optional[List[UUID]] = None
) -> List[ReviewPointDefinitionVersionInDB]:
    """
    Fetches active ReviewPointDefinitionVersion records.
    - If cr_check is True, fetches only the copyright review.
    - If rpd_ids are provided, fetches only those specific RPDs.
    - If neither is provided, returns an empty list.
    """
    # Keep the cr_check logic for backward compatibility. It takes precedence.
    if cr_check:
        query = ReviewPointDefinitionVersion.filter(
            is_active_version=True,
            review_point_definition__is_active=True,
            review_point_definition__key='copyright_review'
        )
        if project_id:
            query = query.filter(
                review_point_definition__project_id=project_id)

        active_versions = await query.prefetch_related('review_point_definition').all()
        return [ReviewPointDefinitionVersionInDB.from_orm(v) for v in active_versions]

    # New logic for rpd_ids
    if not rpd_ids:
        # Return empty list if no IDs are provided (and cr_check is false)
        return []

    query = ReviewPointDefinitionVersion.filter(
        is_active_version=True,
        review_point_definition__is_active=True,
        review_point_definition_id__in=rpd_ids  # Filter by the provided IDs
    )

    if project_id:
        query = query.filter(review_point_definition__project_id=project_id)

    active_versions = await query.prefetch_related('review_point_definition').all()

    return [ReviewPointDefinitionVersionInDB.from_orm(v) for v in active_versions]


async def _generate_single_rpd_findings(
    image_bytes: bytes,
    image_width: int,
    image_height: int,
    text: str,
    rpd_version: ReviewPointDefinitionVersionInDB,
    semaphore: asyncio.Semaphore,
    model_name: str
) -> List[Dict[str, Any]]:
    """
    ä¸ºå•ä¸ªRPDç‰ˆæœ¬ç”Ÿæˆfindingsæ•°æ®ï¼Œä½¿ç”¨asyncio.to_threadå®ç°çœŸæ­£çš„å¹¶è¡Œ
    """
    # æ£€æŸ¥è¯¥rpdæ˜¯å¦readyï¼Œå¦‚æœæ²¡æœ‰readyåˆ™è½®è¯¢ç­‰å¾…
    if not rpd_version.is_ready_for_ai_review:
        print(f"RPDç‰ˆæœ¬ {rpd_version.title} å°šæœªå‡†å¤‡å°±ç»ªï¼Œå¼€å§‹è½®è¯¢ç­‰å¾…...")

        # è½®è¯¢ç­‰å¾…RPDå‡†å¤‡å®Œæˆ
        polling_start_time = time.time()
        max_wait_time = 5 * 60  # 5åˆ†é’Ÿ
        polling_interval = 30   # 30ç§’

        while True:
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            elapsed_time = time.time() - polling_start_time
            if elapsed_time >= max_wait_time:
                print(
                    f"RPDç‰ˆæœ¬ {rpd_version.title} ç­‰å¾…è¶…æ—¶({max_wait_time}ç§’)ï¼Œè¿”å›ç©ºç»“æœ")
                # è¿”å›å›ºå®šçš„ç©ºç»“æœ
                return [{
                    'rpd_version': rpd_version,
                    'finding_dict': {
                        'description': 'RPDå‰å‡¦ç†å¤±æ•—ã—ã¾ã—ãŸ',
                        'severity': 'alert',
                        'suggestion': 'ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„',
                        'tag': 'system_timeout'
                    }
                }]

            # é‡æ–°ä»æ•°æ®åº“è·å–æœ€æ–°çŠ¶æ€
            try:
                fresh_rpd = await ReviewPointDefinitionVersion.get(id=rpd_version.id).prefetch_related('review_point_definition')
                if fresh_rpd.is_ready_for_ai_review:
                    print(f"RPDç‰ˆæœ¬ {rpd_version.title} ç°åœ¨å·²å‡†å¤‡å°±ç»ªï¼Œç»§ç»­å¤„ç†...")
                    # æ›´æ–°rpd_versionå¯¹è±¡ä¸ºæœ€æ–°çŠ¶æ€ï¼Œä¿æŒåŸæœ‰çš„parent_key
                    original_parent_key = rpd_version.parent_key
                    rpd_version = ReviewPointDefinitionVersionInDB.from_orm(
                        fresh_rpd)
                    rpd_version.parent_key = original_parent_key
                    break
                else:
                    print(
                        f"RPDç‰ˆæœ¬ {rpd_version.title} ä»æœªå°±ç»ªï¼Œç­‰å¾…{polling_interval}ç§’åé‡è¯•...")
                    await asyncio.sleep(polling_interval)
            except Exception as e:
                print(f"è½®è¯¢RPDçŠ¶æ€æ—¶å‡ºé”™: {e}")
                await asyncio.sleep(polling_interval)

    # ç«‹å³è®°å½•ä»»åŠ¡å¼€å§‹çš„æ—¶é—´æˆ³
    task_start_time = time.time()
    print(f"*** ä»»åŠ¡ {rpd_version.title},{text} çœŸæ­£å¼€å§‹æ‰§è¡Œ at {task_start_time}")

    start_time = time.time()
    print(f"å¼€å§‹å¤„ç† rpd_version: {rpd_version.title} at {start_time}")

    findings_data = []

    try:
        if rpd_version.parent_key == 'general_ng_review':
            api_start_time = time.time()
            print(f"*** {rpd_version.title} å¼€å§‹APIè°ƒç”¨ at {api_start_time}")

            # ä½¿ç”¨ asyncio.to_thread åœ¨çº¿ç¨‹ä¸­æ‰§è¡ŒåŒæ­¥APIè°ƒç”¨
            findings_list = await asyncio.to_thread(
                _generate_findings_for_ng_review_sync, image_bytes, rpd_version, model_name
            )

            api_end_time = time.time()
            print(
                f"APIè°ƒç”¨å®Œæˆ - {rpd_version.title} at {api_end_time} (è€—æ—¶: {api_end_time - api_start_time:.2f}ç§’)")
            print(f"result: {findings_list}")

            for finding_dict in findings_list:
                findings_data.append({
                    'rpd_version': rpd_version,
                    'finding_dict': {
                        'description': finding_dict['description'],
                        'severity': finding_dict['severity'],
                        'suggestion': finding_dict['suggestion'],
                        'tag': finding_dict.get('tag', '')  # ç¡®ä¿åŒ…å«tagå­—æ®µ
                    }
                })
            if findings_list == []:
                findings_data.append({
                    'rpd_version': rpd_version,
                    'finding_dict': {
                        'description': 'ç‰¹ã«æ³¨æ„ã™ã¹ãã‚‚ã®ã¯ãªã„ã§ã™ã€‚',
                        'severity': 'safe',
                        'suggestion': '',
                        'tag': ''
                    }
                })

        elif rpd_version.parent_key == 'copyright_review':
            api_start_time = time.time()
            print(f"*** {rpd_version.title} å¼€å§‹APIè°ƒç”¨ at {api_start_time}")

            # ä½¿ç”¨æ–°çš„è¾¹ç•Œæ¡†æ£€æµ‹API
            bounding_boxes = await asyncio.to_thread(
                detect_bounding_boxes_new_sync, image_bytes, text
            )
            print(f"bounding_boxes: {bounding_boxes}")
            # é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„bounding box
            if bounding_boxes.bounding_boxes:
                highest_confidence_box = max(
                    bounding_boxes.bounding_boxes, key=lambda box: box.confidence)
                bounding_boxes = BoundingBoxes(
                    bounding_boxes=[highest_confidence_box])
            description, severity, suggestion = await asyncio.to_thread(
                _generate_findings_for_copyright_review_sync, image_bytes, bounding_boxes, rpd_version, model_name
            )

            api_end_time = time.time()
            print(
                f"APIè°ƒç”¨å®Œæˆ - {rpd_version.title} at {api_end_time} (è€—æ—¶: {api_end_time - api_start_time:.2f}ç§’)")
            bbox = convert_bounding_boxes_to_x_y_width_height(
                bounding_boxes, image_width, image_height)
            if description and bbox != []:
                findings_data.append({
                    'rpd_version': rpd_version,
                    'finding_dict': {
                        'description': description,
                        'severity': severity,
                        'suggestion': suggestion
                    },
                    'bounding_boxes': bbox[0]
                })
            elif description:
                findings_data.append({
                    'rpd_version': rpd_version,
                    'finding_dict': {
                        'description': description,
                        'severity': severity,
                        'suggestion': suggestion
                    },
                    'bounding_boxes': {
                        'x': 0,
                        'y': 0,
                        'width': 0,
                        'height': 0
                    }
                })
            else:
                findings_data.append({
                    'rpd_version': rpd_version,
                    'finding_dict': {
                        'description': 'ç‰¹ã«æ³¨æ„ã™ã¹ãã‚‚ã®ã¯ãªã„ã§ã™ã€‚',
                        'severity': 'safe',
                        'suggestion': ''
                    },
                    'bounding_boxes': {
                        'x': 0,
                        'y': 0,
                        'width': 0,
                        'height': 0
                    }
                })

        elif rpd_version.parent_key == 'visual_review':
            if rpd_version.rpd_type != "classification tasks" and rpd_version.rpd_type != "right/wrong tasks":
                print("Unsupported RPD type for visual review:",
                      rpd_version.rpd_type)
                findings_data.append({
                    'rpd_version': rpd_version,
                    'finding_dict': {
                        'description': rpd_version.title,
                        'severity': 'alert',
                        'suggestion': "ä»¥ä¸‹ã®å†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ï¼š" + rpd_version.user_instruction if rpd_version.user_instruction else "ä»¥ä¸‹ã®å†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ï¼š"
                    }
                })
            else:
                api_start_time = time.time()
                print(f"*** {rpd_version.title} å¼€å§‹APIè°ƒç”¨ at {api_start_time}")

                # ä½¿ç”¨æ–°çš„è¾¹ç•Œæ¡†æ£€æµ‹API
                bounding_boxes = await asyncio.to_thread(
                    detect_bounding_boxes_new_sync, image_bytes, text
                )
                print(f"bounding_boxes: {bounding_boxes}")
                # é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„bounding box
                if bounding_boxes.bounding_boxes:
                    highest_confidence_box = max(
                        bounding_boxes.bounding_boxes, key=lambda box: box.confidence)
                    bounding_boxes = BoundingBoxes(
                        bounding_boxes=[highest_confidence_box])
                description, severity, suggestion = await asyncio.to_thread(
                    generate_findings_for_visual_review_multi_agent_sync, image_bytes, rpd_version, bounding_boxes, model_name
                )

                api_end_time = time.time()
                print(
                    f"APIè°ƒç”¨å®Œæˆ - {rpd_version.title} at {api_end_time} (è€—æ—¶: {api_end_time - api_start_time:.2f}ç§’)")

                bbox = convert_bounding_boxes_to_x_y_width_height(
                    bounding_boxes, image_width, image_height)
                if description and bbox != []:
                    findings_data.append({
                        'rpd_version': rpd_version,
                        'finding_dict': {
                            'description': description,
                            'severity': severity,
                            'suggestion': suggestion
                        },
                        'bounding_boxes': bbox[0]
                    })
                elif description:
                    findings_data.append({
                        'rpd_version': rpd_version,
                        'finding_dict': {
                            'description': description,
                            'severity': severity,
                            'suggestion': suggestion
                        },
                        'bounding_boxes': {
                            'x': 0,
                            'y': 0,
                            'width': 0,
                            'height': 0
                        }
                    })
                else:
                    findings_data.append({
                        'rpd_version': rpd_version,
                        'finding_dict': {
                            'description': 'ç‰¹ã«æ³¨æ„ã™ã¹ãã‚‚ã®ã¯ãªã„ã§ã™ã€‚',
                            'severity': 'safe',
                            'suggestion': ''
                        },
                        'bounding_boxes': {
                            'x': 0,
                            'y': 0,
                            'width': 0,
                            'height': 0
                        }
                    })

    except Exception as e:
        print(f"å¤„ç† RPD ç‰ˆæœ¬ {rpd_version.title} æ—¶å‡ºé”™: {e}")
        # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸

    end_time = time.time()
    print(
        f"*** å®Œæˆå¤„ç† rpd_version: {rpd_version.title} at {end_time} (æ€»è€—æ—¶: {end_time - task_start_time:.2f}ç§’)")
    return findings_data


async def _generate_findings_for_all_rpd_versions(
    image_bytes: bytes,
    image_width: int,
    image_height: int,
    content_type: SubtaskType,
    content_metadata: Dict[str, Any],
    active_rpd_versions: List[ReviewPointDefinitionVersionInDB],
    ai_review_orm: AiReview,
    mode: AiReviewMode
) -> List[AiReviewFindingEntry]:
    """
    å¹¶è¡Œä¸ºæ‰€æœ‰RPDç‰ˆæœ¬ç”Ÿæˆfindingsï¼Œç„¶åæ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“
    """
    start_time = time.time()

    model_name = QUALITY_MODEL if mode == AiReviewMode.QUALITY else SPEED_MODEL
    logger.info(
        f"Using model: {model_name} for AI review in mode: {mode.value}")

    # è®¾ç½®æ›´å¤§çš„å¹¶å‘æ•°ï¼Œå› ä¸ºç°åœ¨æ¯ä¸ªä»»åŠ¡éƒ½æœ‰ç‹¬ç«‹çš„å®¢æˆ·ç«¯
    semaphore = asyncio.Semaphore(MAX_CONCURRENT)

    print(f"å¼€å§‹å¤„ç† {len(active_rpd_versions)} ä¸ªRPDç‰ˆæœ¬")

    # æ£€æŸ¥ä¸­æ–­ä¿¡å·
    if await _check_cancellation_signal(ai_review_orm):
        print("æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢å¤„ç†")
        return []

    # RPD è¿‡æ»¤é˜¶æ®µ
    # filtered_rpd_versions = active_rpd_versions
    filtered_rpd_versions = []

    try:
        print(f"å¼€å§‹ RPD è¿‡æ»¤ï¼ŒåŸå§‹ RPD æ•°é‡: {len(active_rpd_versions)}")

        # æ£€æŸ¥ä¸­æ–­ä¿¡å·
        if await _check_cancellation_signal(ai_review_orm):
            print("åœ¨ RPD è¿‡æ»¤å‰æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢å¤„ç†")
            return []

        # æ‰§è¡Œ RPD è¿‡æ»¤ - ç›´æ¥ä½¿ç”¨ ReviewPointDefinitionVersionInDBï¼Œæ— éœ€è½¬æ¢
        filter_start_time = time.time()

        # åˆ›å»ºå–æ¶ˆæ£€æŸ¥å›è°ƒå‡½æ•°
        async def cancellation_check():
            return await _check_cancellation_signal(ai_review_orm)

        filtered_rpd_versions_raw = await filter_rpd_async(
            image_bytes,
            active_rpd_versions,  # ç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€è½¬æ¢
            max_concurrent=MAX_CONCURRENT,  # ä¸å¹¶è¡Œå¤„ç†ä¿æŒä¸€è‡´çš„å¹¶å‘æ•°
            cancellation_check_callback=cancellation_check
        )
        filter_end_time = time.time()
        print(f"RPD è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {filter_end_time - filter_start_time:.2f}ç§’")
        print(f"è¿‡æ»¤å RPD æ•°é‡: {len(filtered_rpd_versions_raw)}")

        # æ£€æŸ¥ä¸­æ–­ä¿¡å·
        if await _check_cancellation_signal(ai_review_orm):
            print("åœ¨ RPD è¿‡æ»¤åæ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢å¤„ç†")
            return []

        # ç›´æ¥ä½¿ç”¨è¿‡æ»¤åçš„ç»“æœï¼Œæ— éœ€è½¬æ¢
        filtered_rpd_versions = filtered_rpd_versions_raw

        print(f"æœ€ç»ˆç”¨äºå¤„ç†çš„ RPD æ•°é‡: {len(filtered_rpd_versions)}")

    except Exception as e:
        print(f"RPD è¿‡æ»¤è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print("å°†ä½¿ç”¨åŸå§‹ RPD åˆ—è¡¨ç»§ç»­å¤„ç†")
        # filtered_rpd_versions = active_rpd_versions
        for rpd_version in active_rpd_versions:
            filtered_rpd_versions.append({
                'rpd_version': rpd_version,
                'word': rpd_version.title
            })

    # å¦‚æœè¿‡æ»¤åæ²¡æœ‰ RPDï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
    if not filtered_rpd_versions:
        print("è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆçš„ RPDï¼Œè·³è¿‡å¤„ç†")
        return []

    print(f"å¼€å§‹å¹¶è¡Œå¤„ç† {len(filtered_rpd_versions)} ä¸ªè¿‡æ»¤åçš„RPDç‰ˆæœ¬")

    # åˆ›å»ºä»»åŠ¡ - ç«‹å³å¯åŠ¨æ‰€æœ‰ä»»åŠ¡
    print("åˆ›å»ºå¹¶ç«‹å³å¯åŠ¨æ‰€æœ‰ä»»åŠ¡...")
    tasks = []
    for i, rpd_version_dict in enumerate(filtered_rpd_versions):
        rpd_version = rpd_version_dict['rpd_version']
        text = rpd_version_dict['word']
        print(f"ä¸º {rpd_version.title} åˆ›å»ºä»»åŠ¡ {i} at {time.time()}")
        # ä½¿ç”¨ asyncio.create_task ç«‹å³å¯åŠ¨ä»»åŠ¡
        task = asyncio.create_task(
            _generate_single_rpd_findings(
                image_bytes,  image_width, image_height, text, rpd_version, semaphore, model_name),
            name=f"rpd_task_{rpd_version.title}"
        )
        tasks.append(task)
        print(f"ä»»åŠ¡ {rpd_version.title} å·²åˆ›å»ºå¹¶å¯åŠ¨ at {time.time()}")

    print(f"æ‰€æœ‰ {len(tasks)} ä¸ªä»»åŠ¡å·²åˆ›å»ºå¹¶å¯åŠ¨ï¼Œå¼€å§‹ç­‰å¾…å®Œæˆ...")

    # ä½¿ç”¨å¯ä¸­æ–­çš„ç­‰å¾…æœºåˆ¶
    done = await _wait_for_tasks_with_cancellation(tasks, ai_review_orm, 2.0)

    # å¦‚æœè¿”å›ç©ºåˆ—è¡¨ï¼Œè¯´æ˜è¢«ä¸­æ–­äº†
    if not done:
        return []

    parallel_time = time.time()
    print(f"å¹¶è¡Œå¤„ç†å®Œæˆï¼Œè€—æ—¶: {parallel_time - start_time:.2f}ç§’")

    # å¤„ç†ç»“æœå¹¶åˆ›å»ºæ•°æ®åº“å¯¹è±¡
    all_findings = []
    for task in done:
        try:
            findings_data_list = await task
            if isinstance(findings_data_list, Exception):
                print(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {findings_data_list}")
                continue

            for finding_data in findings_data_list:
                rpd_version = finding_data['rpd_version']
                finding_dict = finding_data['finding_dict']
                if 'bounding_boxes' in finding_data:
                    area = finding_data['bounding_boxes']
                else:
                    area = {'x': 0, 'y': 0, 'width': 0, 'height': 0}
                # åˆ›å»ºå®Œæ•´çš„AiReviewFindingEntryå¯¹è±¡
                finding_entry = await AiReviewFindingEntry.create(
                    ai_review=ai_review_orm,
                    review_point_definition_version_id=rpd_version.id,
                    description=finding_dict['description'],
                    severity=finding_dict['severity'],
                    suggestion=finding_dict.get('suggestion', ''),
                    area=area,  # é»˜è®¤åŒºåŸŸ
                    is_ai_generated=True,
                    status='pending_human_review',
                    tag=finding_dict.get('tag', ''),
                    reference_images=rpd_version.reference_images,
                    reference_source=rpd_version.title,
                    content_type=content_type,
                    content_metadata=content_metadata
                )
                all_findings.append(finding_entry)
        except Exception as e:
            print(f"å¤„ç†ä»»åŠ¡ç»“æœæ—¶å‡ºé”™: {e}")

    db_end_time = time.time()
    print(
        f"æ•°æ®åº“æ“ä½œè€—æ—¶: {db_end_time - parallel_time:.2f}ç§’, æ€»è€—æ—¶: {db_end_time - start_time:.2f}ç§’")

    return all_findings


async def _process_single_video_frame(
    video_frame_info: Dict[str, Any],
    video_width: int,
    video_height: int,
    content_type: SubtaskType,
    active_rpd_versions: List[ReviewPointDefinitionVersionInDB],
    ai_review_orm: AiReview,
    mode: AiReviewMode,
    semaphore: asyncio.Semaphore
) -> List[AiReviewFindingEntry]:
    """
    å¤„ç†å•ä¸ªè§†é¢‘å¸§ï¼Œä¸ºå…¶ç”Ÿæˆæ‰€æœ‰RPDçš„findings
    """
    async with semaphore:
        try:
            video_frame_bytes = video_frame_info["middle_frame_bytes"]
            content_metadata = {
                "scene_number": video_frame_info["scene_number"],
                "start_timestamp": video_frame_info["start_time"],
                "end_timestamp": video_frame_info["end_time"],
                "start_frame": video_frame_info["start_frame"],
                "end_frame": video_frame_info["end_frame"],
            }

            print(f"å¤„ç†è§†é¢‘å¸§ - åœºæ™¯ {content_metadata['scene_number']}")

            # ä¸ºè¿™ä¸ªè§†é¢‘å¸§ç”Ÿæˆæ‰€æœ‰RPDçš„findings
            findings = await _generate_findings_for_all_rpd_versions(
                video_frame_bytes, video_width, video_height, content_type,
                content_metadata, active_rpd_versions, ai_review_orm, mode
            )

            print(
                f"è§†é¢‘å¸§åœºæ™¯ {content_metadata['scene_number']} å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(findings)} ä¸ªfindings")
            return findings

        except Exception as e:
            print(f"å¤„ç†è§†é¢‘å¸§æ—¶å‡ºé”™: {e}")
            return []


async def _generate_findings_for_video_frames_parallel(
    video_data: List[Dict[str, Any]],
    video_width: int,
    video_height: int,
    content_type: SubtaskType,
    active_rpd_versions: List[ReviewPointDefinitionVersionInDB],
    ai_review_orm: AiReview,
    mode: AiReviewMode,
    max_concurrent: Optional[int] = None
) -> List[AiReviewFindingEntry]:
    """
    å¹¶è¡Œå¤„ç†æ‰€æœ‰è§†é¢‘å¸§ï¼Œä¸ºæ¯ä¸ªå¸§ç”ŸæˆAIå®¡æŸ¥findings
    """
    start_time = time.time()

    print(f"å¼€å§‹å¹¶è¡Œå¤„ç† {len(video_data)} ä¸ªè§†é¢‘å¸§")

    # è®¾ç½®å¹¶å‘ä¿¡å·é‡
    if max_concurrent is None:
        max_concurrent = MAX_CONCURRENT
    semaphore = asyncio.Semaphore(max_concurrent)

    # åˆ›å»ºæ‰€æœ‰è§†é¢‘å¸§å¤„ç†ä»»åŠ¡
    tasks = []
    for i, video_frame_info in enumerate(video_data):
        task = asyncio.create_task(
            _process_single_video_frame(
                video_frame_info, video_width, video_height, content_type,
                active_rpd_versions, ai_review_orm, mode, semaphore
            ),
            name=f"video_frame_task_{i}"
        )
        tasks.append(task)

    print(f"æ‰€æœ‰ {len(tasks)} ä¸ªè§†é¢‘å¸§ä»»åŠ¡å·²åˆ›å»ºï¼Œå¼€å§‹ç­‰å¾…å®Œæˆ...")

    # ä½¿ç”¨å¯ä¸­æ–­çš„ç­‰å¾…æœºåˆ¶
    done = await _wait_for_tasks_with_cancellation(tasks, ai_review_orm, 2.0)

    # å¦‚æœè¿”å›ç©ºåˆ—è¡¨ï¼Œè¯´æ˜è¢«ä¸­æ–­äº†
    if not done:
        return []

    # æ”¶é›†æ‰€æœ‰ç»“æœ
    all_findings = []
    for task in done:
        try:
            findings = await task
            all_findings.extend(findings)
        except Exception as e:
            print(f"è·å–è§†é¢‘å¸§å¤„ç†ç»“æœæ—¶å‡ºé”™: {e}")

    end_time = time.time()
    print(f"è§†é¢‘å¸§å¹¶è¡Œå¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
    print(f"å…±ç”Ÿæˆ {len(all_findings)} ä¸ªfindings")

    return all_findings

# ------------------------------------------------------------
# ç§°å‘¼æ£€æŸ¥ (moved from check_speakers.py)
# ------------------------------------------------------------

# Speaker check related models (moved from check_speakers.py)


class SpeakResult(BaseModel):
    speaker: str = Field(description="The official name of the speaker")
    speaker_used_name: str = Field(
        description="The actual name used for the speaker in the conversation")


class TargetResult(BaseModel):
    target: List[str] = Field(
        description="List of official names of people mentioned in the conversation")
    target_used_names: List[str] = Field(
        description="List of actual names used in the conversation for each target")


class AliasCheckResult(BaseModel):
    wrong_list: List[str] = Field(description="List of wrong alias")
    correct_list: List[str] = Field(description="List of correct alias")


async def get_speaker(
    gemini_client: Any,
    cleaned_text: str,
    model_name: str,
    alias_dict: Dict[str, Dict[str, str]] = None,
    max_retries: int = 3,
    retry_delay: float = 1.0,
    timeout: float = 30.0,  # æ·»åŠ è¶…æ—¶å‚æ•°,
) -> SpeakResult:
    """ç¬¬ä¸€æ­¥ï¼šè·å–speaker"""
    # print(f"ç¬¬ä¸€æ­¥ï¼šåˆ†æå¯¹è¯ä¸­çš„è¯´è¯è€…: {cleaned_text}")

    # è·å–æ‰€æœ‰å¯èƒ½çš„è¯´è¯è€…
    official_speakers = sorted(list(alias_dict.keys()))

    for attempt in range(max_retries + 1):
        try:
            # print(f"ğŸ”„ ç¬¬ä¸€æ­¥ API è°ƒç”¨å°è¯• {attempt + 1}/{max_retries + 1}")

            response = await asyncio.wait_for(
                asyncio.to_thread(
                    gemini_client.models.generate_content,
                    model=model_name,
                    contents=cleaned_text,
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json",
                        response_schema=SpeakResult,
                        system_instruction=get_speaker_prompt.replace(
                            "{json.dumps(official_speakers, ensure_ascii=False)}",
                            json.dumps(official_speakers, ensure_ascii=False)
                        ),
                        temperature=0.0,
                    )
                ),
                timeout=timeout
            )

            result = response.parsed
            return result

        except asyncio.TimeoutError:
            print(f"â° ç¬¬ä¸€æ­¥APIè°ƒç”¨è¶…æ—¶ (å°è¯• {attempt + 1}), è¶…æ—¶æ—¶é—´: {timeout}ç§’")
            if attempt == max_retries:
                raise Exception(f"ç¬¬ä¸€æ­¥APIè°ƒç”¨åœ¨ {max_retries + 1} æ¬¡å°è¯•åä»ç„¶è¶…æ—¶")

        except Exception as e:
            print(f"âŒ ç¬¬ä¸€æ­¥APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}): {str(e)}")
            if attempt == max_retries:
                raise e

        if attempt < max_retries:
            print(f"â³ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
            await asyncio.sleep(retry_delay)
            retry_delay *= 1.5

    return None


async def get_targets(
    gemini_client: Any,
    line: str,
    speaker: str,
    model_name: str,
    alias_dict: Dict[str, Dict[str, str]],
    all_characters: List[str],
    max_retries: int = 3,
    retry_delay: float = 1.0,
    timeout: float = 30.0,
    sheet_index: int = 0,
) -> TargetResult:
    """ç¬¬äºŒæ­¥ï¼šè·å–å¯¹è¯ä¸­æåˆ°çš„äººç‰©"""
    # print(f"ğŸ” ç¬¬äºŒæ­¥ï¼šåˆ†æå¯¹è¯ä¸­çš„ç›®æ ‡äººç‰©(è½®æ¬¡: {sheet_index})")

    # æ„é€ æ‰€æœ‰å¯èƒ½çš„ç§°å‘¼è§„åˆ™ (ä¸å«speaker)
    all_possible_aliases = set()
    for target, alias in alias_dict.get(speaker, {}).items():
        all_possible_aliases.add(f"{target} should be called as '{alias}'")

    possible_aliases_text = "".join(sorted(list(all_possible_aliases)))

    # æ ¹æ®è½®æ¬¡åŠ¨æ€æ·»åŠ æŒ‡ä»¤

    for attempt in range(max_retries + 1):
        try:
            # print(f"ğŸ”„ ç¬¬äºŒæ­¥ API è°ƒç”¨å°è¯• {attempt + 1}/{max_retries + 1}")

            response = await asyncio.wait_for(
                asyncio.to_thread(
                    gemini_client.models.generate_content,
                    model=model_name,
                    contents=line,
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json",
                        response_schema=TargetResult,
                        system_instruction=get_target_prompt.replace(
                            "{speaker}", speaker
                        ).replace(
                            "{possible_aliases_text}", possible_aliases_text
                        ).replace(
                            "{json.dumps(all_characters, ensure_ascii=False)}",
                            json.dumps(all_characters, ensure_ascii=False)
                        ),
                        temperature=0.0,
                    )
                ),
                timeout=timeout
            )

            result = response.parsed
            return result, possible_aliases_text

        except asyncio.TimeoutError:
            print(f"â° ç¬¬äºŒæ­¥APIè°ƒç”¨è¶…æ—¶ (å°è¯• {attempt + 1}), è¶…æ—¶æ—¶é—´: {timeout}ç§’")
            if attempt == max_retries:
                raise Exception("ç¬¬äºŒæ­¥APIè°ƒç”¨è¶…æ—¶")

        except Exception as e:
            print(f"âŒ ç¬¬äºŒæ­¥APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}): {str(e)}")
            if attempt == max_retries:
                raise e

        if attempt < max_retries:
            await asyncio.sleep(retry_delay)
            retry_delay *= 1.5
    return None, None


async def check_speaker(
    data: Dict[str, Any],
    model_name: str,
    max_retries: int = 3,
    retry_delay: float = 1.0,
    alias_dict: Dict[str, Dict[str, str]] = None,
    extra_info: List[Dict[str, Any]] = None,
    api_timeout: float = 30.0,  # æ·»åŠ APIè¶…æ—¶å‚æ•°
    sheet_index: int = 0
) -> Dict[str, Any]:
    api_start_time = time.time()

    # å¦‚æœ extra_info æœªæä¾›ï¼Œåˆ™åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
    if extra_info is None:
        extra_info = []

    try:
        gemini_client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])

        # é¢„å¤„ç†æ–‡æœ¬
        original_index = data['original_index']  # ä½¿ç”¨æ­£ç¡®çš„é”®å
        speaker = data['speaker']
        line = data['line']
        content = f"ç™ºè©±è€…:{speaker}, ä¼šè©±å†…å®¹:{line}"

        # print(f"ğŸ“ å¤„ç†æ®µè½ {original_index}: {content}")

        # ç¬¬ä¸€æ­¥ï¼šè·å–speaker
        speaker_result = await get_speaker(
            gemini_client,
            content,
            model_name,
            alias_dict,
            max_retries,
            retry_delay,
            api_timeout,
        )

        # ç¬¬ä¸€è½®ç»“æœ
        first_result = {
            "original_index": original_index,  # ä½¿ç”¨æ­£ç¡®çš„é”®å
            "speaker": speaker_result.speaker.replace("é³³ãƒ»ã“ã“ãª", "é³³Â·ã“ã“ãª"),
            "line": line,
            "speaker_used_name": speaker_result.speaker_used_name
        }

        # è·å–æ‰€æœ‰å¯èƒ½çš„è§’è‰²åˆ—è¡¨
        all_characters = list(alias_dict.keys())

        # ç¬¬äºŒæ­¥ï¼šè·å–target
        target_result, possible_aliases_text = await get_targets(
            gemini_client,
            line,
            speaker_result.speaker,
            model_name,
            alias_dict,
            all_characters,
            max_retries,
            retry_delay,
            api_timeout,
            sheet_index=sheet_index,

        )

        second_result = {
            "original_index": original_index,  # ä½¿ç”¨æ­£ç¡®çš„é”®å
            "target": [target.replace("é³³ãƒ»ã“ã“ãª", "é³³Â·ã“ã“ãª") for target in target_result.target],
            "target_used_names": target_result.target_used_names,
        }

        # åˆå¹¶ä¸¤è½®ç»“æœ
        combined_result = {
            **first_result,
            **second_result,
        }

        # ç¬¬ä¸‰æ­¥: æœºæ¢°æ£€æŸ¥åˆ«åä½¿ç”¨ (æ–°ç‰ˆä¸‰çº§æ£€æŸ¥é€»è¾‘)

        # å¦‚æœè¯´è¯è€…ä¸åœ¨ç§°å‘¼è¡¨ä¸­ï¼Œåˆ™è·³è¿‡æ£€æŸ¥ï¼Œä½†æ ‡è®°ä¸ºunknown_speakerçŠ¶æ€
        speaker_name = combined_result["speaker"]
        if speaker_name not in alias_dict:
            # print(f"â„¹ï¸ è¯´è¯è€…ã€Œ{speaker_name}ã€ä¸åœ¨ç§°å‘¼è¡¨ (alias_dict) ä¸­ï¼Œè·³è¿‡æœºæ¢°æ£€æŸ¥ã€‚")
            return {
                **combined_result,
                "status": "unknown_speaker",
                "message": f"è©±è€…ã€Œ{speaker_name}ã€ã¯å‘¼ç§°è¡¨ã«ã‚ã‚Šã¾ã›ã‚“ã€‚",
                "response_time": round(time.time() - api_start_time, 2),
                "error": None
            }

        found_errors = []
        found_warnings = []

        speaker_alias_rules = alias_dict.get(combined_result["speaker"], {})

        for i, target in enumerate(combined_result["target"]):
            if i >= len(combined_result["target_used_names"]):
                continue

            used_name = combined_result["target_used_names"][i]
            speaker_name = combined_result["speaker"]

            # 1. å¸¸è§„è§„åˆ™æ£€æŸ¥
            correct_alias = speaker_alias_rules.get(target)
            if correct_alias and (used_name == correct_alias or used_name == target):
                # print(f"âœ… (å¸¸è§„) æ£€æŸ¥é€šè¿‡: {speaker_name} å¯¹ {target} ä½¿ç”¨ '{used_name}' æ˜¯æ­£ç¡®çš„ã€‚")
                continue

            # 2. ç‰¹æ®Šè§„åˆ™æ£€æŸ¥
            is_special_case = False
            if extra_info:
                for rule in extra_info:
                    if (rule.get('speaker') == speaker_name and
                        rule.get('target') == target and
                            rule.get('alias') == used_name):

                        conditions = rule.get('conditions', ['ç„¡ç‰¹å®šæ¡ä»¶'])
                        conditions_text = ', '.join(conditions) if isinstance(
                            conditions, list) else str(conditions)
                        message = f"[{speaker_name}] â†’ [{target}] : ç‰¹æ®Šãªå‘¼ã³æ–¹ã€Œ{used_name}ã€ãŒä½¿ã‚ã‚Œã¾ã—ãŸã€‚æ¡ä»¶:{conditions_text}ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                        found_warnings.append(message)
                        # print(f"âš ï¸ (ç‰¹æ®Š) æ£€æŸ¥é€šè¿‡: {speaker_name} å¯¹ {target} ä½¿ç”¨ '{used_name}' (æ¡ä»¶: {conditions_text})")
                        is_special_case = True
                        break  # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…çš„ç‰¹æ®Šè§„åˆ™å°±å¤Ÿäº†

            if is_special_case:
                # todo ç‰¹æ®Šè¦å‰‡å¯¾å¿œä¸å¯
                continue

            # 3. å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œåˆ™ä¸ºé”™è¯¯
            if correct_alias:
                message = f"{speaker_name} â†’ {target} ã®å‘¼ã³æ–¹ã¯ã€Œ{correct_alias}ã€ã§ã™ãŒã€ã€Œ{used_name}ã€ãŒä½¿ã‚ã‚Œã¾ã—ãŸã€‚"
            else:
                message = f"{speaker_name} â†’ {target} ã®å‘¼ã³æ–¹ãŒã‚ã‚Šã¾ã›ã‚“ãŒã€ã€Œ{used_name}ã€ãŒä½¿ã‚ã‚Œã¾ã—ãŸã€‚"

            found_errors.append(message)
            # print(f"âŒ å‘ç°é”™è¯¯: {message}")

        # å†³å®šæœ€ç»ˆçŠ¶æ€å’Œæ¶ˆæ¯
        # çŠ¶æ€åˆ†ç±»ï¼š
        # - "ok": å®Œå…¨æ­£ç¡®ï¼Œæ— é—®é¢˜
        # - "warning": ç‰¹æ®Šè§„åˆ™æƒ…å†µï¼Œéœ€è¦ç¡®è®¤
        # - "error": é”™è¯¯ï¼Œéœ€è¦ä¿®æ­£
        final_status = "ok"
        final_message = f"ã‚»ãƒªãƒ•ï¼š{speaker}: {line} - å•é¡Œãªã—"

        if found_errors:
            final_status = "error"
            # é”™è¯¯ä¿¡æ¯ä¼˜å…ˆï¼ŒåŒæ—¶é™„å¸¦è­¦å‘Šä¿¡æ¯
            final_message = "\n".join(found_errors + found_warnings)
        elif found_warnings:
            # å¦‚æœåªæœ‰è­¦å‘Šï¼Œä½¿ç”¨ä¸“é—¨çš„ç‰¹æ®Šè§„åˆ™çŠ¶æ€
            final_status = "warning"
            final_message = "\n".join(found_warnings)

        # æœ€ç»ˆç»“æœ
        final_result = {
            **combined_result,
            "status": final_status,
            "message": final_message,
            "response_time": round(time.time() - api_start_time, 2),
            "error": None
        }

        return final_result

    except Exception as e:
        error_time = round(time.time() - api_start_time, 2)
        print(
            f"âŒ æ®µè½ {data.get('original_index', 'unknown')} å¤„ç†å¤±è´¥ï¼Œè€—æ—¶: {error_time}ç§’")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        print("è¯¦ç»†è¿½æº¯ä¿¡æ¯:")
        traceback.print_exc()
        return {
            "original_index": data.get('original_index', 'unknown'),  # ä½¿ç”¨æ­£ç¡®çš„é”®å
            "speaker": data.get('speaker', ''),
            "line": data.get('line', ''),
            "flag": False,
            "wrong_list": [],
            "correct_list": [],
            "response_time": error_time,
            "error": f"{type(e).__name__}: {e}{traceback.format_exc()}"
        }


def _load_appellation_from_rpds(text_rpds: List[ReviewPointDefinitionVersionInDB]) -> Dict[str, Dict[str, str]]:
    """
    ä»text_review RPDçš„reference_filesä¸­åŠ è½½ç§°å‘¼è¡¨

    Args:
        text_rpds: text_reviewç±»å‹çš„RPDç‰ˆæœ¬åˆ—è¡¨

    Returns:
        Dict[str, Dict[str, str]]: ç§°å‘¼è¡¨æ•°æ®
    """
    alias_dict = {}
    for rpd in text_rpds:
        if rpd.reference_files:
            # æŸ¥æ‰¾JSONæ–‡ä»¶ï¼ˆç§°å‘¼è¡¨ï¼‰
            appellation_file = None
            for file_url in rpd.reference_files:
                if file_url.endswith('.json'):
                    appellation_file = file_url
                    break

            if appellation_file:
                print(f"ä»RPD {rpd.title} åŠ è½½ç§°å‘¼è¡¨: {appellation_file}")
                loaded_dict = _load_config_from_s3(appellation_file, "ç§°å‘¼è¡¨")
                if loaded_dict:
                    alias_dict.update(loaded_dict)
                    break  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæˆåŠŸåŠ è½½çš„ç§°å‘¼è¡¨
            else:
                print(f"RPD {rpd.title} çš„reference_filesä¸­æœªæ‰¾åˆ°JSONæ–‡ä»¶")

    return alias_dict


def _load_config_from_s3(s3_path: str, description: str) -> Any:
    """ä»S3åŠ è½½é…ç½®æ–‡ä»¶çš„é€šç”¨å‡½æ•°"""
    try:
        from aimage_supervision.clients.aws_s3 import \
            download_file_content_from_s3_sync
        from aimage_supervision.settings import AWS_BUCKET_NAME

        print(f"æ­£åœ¨ä»S3åŠ è½½{description}: {s3_path}")

        # è§£æS3è·¯å¾„ï¼Œæå–bucketåç§°å’Œæ–‡ä»¶è·¯å¾„
        if s3_path.startswith('s3://'):
            # å®Œæ•´S3 URLæ ¼å¼ï¼šs3://bucket-name/path/to/file
            parts = s3_path.replace('s3://', '').split('/', 1)
            if len(parts) == 2:
                bucket_name = parts[0]
                file_path = parts[1]
                print(f"è§£æS3 URL - Bucket: {bucket_name}, æ–‡ä»¶è·¯å¾„: {file_path}")
            else:
                # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤bucketå’ŒåŸè·¯å¾„
                bucket_name = AWS_BUCKET_NAME
                file_path = s3_path
                print(f"S3 URLè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤bucket: {bucket_name}")
        else:
            # ç›¸å¯¹è·¯å¾„æ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤bucket
            bucket_name = AWS_BUCKET_NAME
            file_path = s3_path
            print(f"ä½¿ç”¨ç›¸å¯¹è·¯å¾„å’Œé»˜è®¤bucket: {bucket_name}")

        # ä½¿ç”¨è§£æå‡ºçš„bucketåç§°å’Œæ–‡ä»¶è·¯å¾„ä¸‹è½½æ–‡ä»¶
        file_content = download_file_content_from_s3_sync(
            file_path, bucket_name)
        content_str = file_content.decode('utf-8')
        config_data = json.loads(content_str)
        print(f"æˆåŠŸåŠ è½½{description}ï¼Œæ•°æ®å¤§å°: {len(content_str)} å­—ç¬¦")
        return config_data
    except ValueError as e:
        # download_file_content_from_s3_sync ä¼šåœ¨æ–‡ä»¶ä¸å­˜åœ¨æ—¶æŠ›å‡º ValueError
        if "Error downloading file from S3" in str(e):
            print(f"S3æ–‡ä»¶ä¸å­˜åœ¨: {s3_path}")
        else:
            print(f"ä»S3åŠ è½½{description}å¤±è´¥: {e}")
        return None
    except Exception as e:
        print(f"ä»S3åŠ è½½{description}å¤±è´¥: {e}")
        return None


async def _process_single_text_part(
    prepared_data: Dict[str, Any],
    content_type: SubtaskType,
    text_rpds: List[ReviewPointDefinitionVersionInDB],
    ai_review_orm: AiReview,
    alias_dict: Dict[str, Any],
    semaphore: asyncio.Semaphore,
    model_name: str,
    extra_info: List[Dict[str, Any]] = None
) -> List[AiReviewFindingEntry]:
    """
    å¤„ç†å•ä¸ªæ–‡æœ¬éƒ¨åˆ†ï¼Œä¸ºå…¶ç”Ÿæˆæ‰€æœ‰RPDçš„findingsï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    æ•´åˆäº†_load_alias_dict_once, _determine_severity, _create_findings_from_check_resultçš„åŠŸèƒ½

    Args:
        prepared_data: é¢„å¤„ç†çš„æ–‡æœ¬æ•°æ®
        content_type: å†…å®¹ç±»å‹
        text_rpds: æ–‡æœ¬å®¡æŸ¥çš„RPDç‰ˆæœ¬åˆ—è¡¨
        ai_review_orm: AIå®¡æŸ¥ORMå¯¹è±¡
        alias_dict: å‘¼ç§°è¡¨å­—å…¸
        semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        mode: AIå®¡æŸ¥æ¨¡å¼ (QUALITY/SPEED)ï¼Œæ§åˆ¶æ£€æŸ¥ä¸¥æ ¼ç¨‹åº¦
        extra_info: ç‰¹æ®Šè§„åˆ™åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºç©ºåˆ—è¡¨
                   æ ¼å¼: [{"speaker": "è§’è‰²A", "target": "è§’è‰²B", "alias": "ç‰¹æ®Šç§°å‘¼", "conditions": ["æ¡ä»¶1", "æ¡ä»¶2"]}]
    """
    async with semaphore:
        try:
            # å¦‚æœæ²¡æœ‰å‘¼ç§°è¡¨æˆ–æ²¡æœ‰text_rpdsï¼Œç›´æ¥è¿”å›
            if not alias_dict or not text_rpds:
                return []

            # æ‰§è¡Œè¯´è¯è€…/ç§°å‘¼æ£€æŸ¥ - ç›´æ¥ä½¿ç”¨é¢„å¤„ç†çš„æ•°æ®
            check_result = await check_speaker(
                data=prepared_data,
                model_name=model_name,
                alias_dict=alias_dict,
                extra_info=extra_info or [],
                sheet_index=prepared_data["sheet_index"]
            )

            # ã€æ•´åˆ_determine_severityåŠŸèƒ½ã€‘ç¡®å®šä¸¥é‡ç¨‹åº¦ - ä¸statusç»Ÿä¸€
            # æ˜ å°„å…³ç³»ï¼ˆå‰ç«¯ä½¿ç”¨["risk", "alert", "safe"]ï¼‰ï¼š
            # - status="ok" â†’ severity="safe" (å®Œå…¨æ­£ç¡®)
            # - status="warning" â†’ severity="alert" (ç‰¹æ®Šè§„åˆ™ï¼Œéœ€ç¡®è®¤)
            # - status="error" â†’ severity="risk" (é”™è¯¯ï¼Œéœ€ä¿®æ­£)
            # - status="unknown_speaker" â†’ severity="alert" (speakerä¸åœ¨ç§°å‘¼è¡¨)
            # - status="unknown_target" â†’ severity="alert" (targetä¸åœ¨ç§°å‘¼è¡¨)
            status_val = check_result.get("status", "error")
            message_val = check_result.get("message", "") or ""

            if status_val == "error":
                severity = "risk"
            elif status_val == "warning":  # ç‰¹æ®Šè§„åˆ™æƒ…å†µ
                severity = "alert"
            elif status_val in ["unknown_speaker", "unknown_target"]:  # speakeræˆ–targetä¸åœ¨ç§°å‘¼è¡¨
                severity = "alert"
            elif status_val == "ok":
                severity = "safe"
            else:
                severity = "risk"  # æœªçŸ¥çŠ¶æ€é»˜è®¤ä¸ºrisk

            # ã€æ•´åˆ_create_findings_from_check_resultåŠŸèƒ½ã€‘æ ¹æ®æ£€æŸ¥ç»“æœåˆ›å»ºfindings
            messages = [m.strip()
                        for m in message_val.split("\n") if m.strip()]

            # æ‰©å±•content_metadataï¼ŒåŒ…å«check_speakerçš„å®Œæ•´ç»“æœ
            content_metadata = {
                "original_index": prepared_data["original_index"],
                "sheet_index": prepared_data["sheet_index"],
                "speaker": prepared_data["speaker"],
                "line": prepared_data["line"],
                # ä¿å­˜check_speakerçš„å®Œæ•´ç»“æœ
                "check_speaker_result": {
                    "detected_speaker": check_result.get("speaker", prepared_data["speaker"]),
                    # æ³¨æ„ï¼šcheck_speakerè¿”å›çš„æ˜¯"target"å­—æ®µ
                    "target_list": check_result.get("target", []),
                    "target_used_names": check_result.get("target_used_names", []),
                    "speaker_used_name": check_result.get("speaker_used_name", ""),
                    "status": check_result.get("status", ""),
                }
            }

            created_findings = []
            for rpd_version in text_rpds:
                for msg in messages:
                    try:
                        finding_entry = await AiReviewFindingEntry.create(
                            ai_review=ai_review_orm,
                            review_point_definition_version_id=rpd_version.id,
                            description=msg,
                            severity=severity,
                            suggestion="",
                            area={"x": 0, "y": 0, "width": 0, "height": 0},
                            is_ai_generated=True,
                            status='pending_human_review',
                            tag='speaker_check',
                            reference_images=rpd_version.reference_images,
                            reference_source=rpd_version.title,
                            content_type=content_type,
                            content_metadata=content_metadata,
                        )
                        created_findings.append(finding_entry)
                    except Exception as create_err:
                        print(f"åˆ›å»ºæ–‡æœ¬ finding å¤±è´¥: {create_err}")

            print(
                f"æ–‡æœ¬éƒ¨åˆ† sheet {prepared_data.get('sheet_index', 'unknown')}, row {prepared_data.get('original_index', 'unknown')} å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(created_findings)} ä¸ªfindings"
            )
            return created_findings

        except NotImplementedError:
            print(
                f"æ–‡æœ¬å¤„ç†åŠŸèƒ½å°šæœªå®ç°ï¼Œè·³è¿‡æ–‡æœ¬éƒ¨åˆ† {prepared_data.get('original_index', 'unknown')}")
            return []
        except Exception as e:
            print(f"å¤„ç†æ–‡æœ¬éƒ¨åˆ†æ—¶å‡ºé”™: {e}")
            return []


async def _generate_findings_for_text_parts_parallel(
    text_parts: List[Dict[str, Any]],
    content_type: SubtaskType,
    active_rpd_versions: List[ReviewPointDefinitionVersionInDB],
    ai_review_orm: AiReview,
    mode: AiReviewMode,
    local_excel_path: Optional[str] = None,
    original_s3_path: Optional[str] = None,
    max_concurrent: Optional[int] = None,
    extra_info: List[Dict[str, Any]] = None
) -> List[AiReviewFindingEntry]:
    """
    å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡æœ¬éƒ¨åˆ†ï¼Œä¸ºæ¯ä¸ªéƒ¨åˆ†ç”ŸæˆAIå®¡æŸ¥findingsï¼Œå¹¶æ›´æ–°Excelæ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

    Args:
        text_parts: æ–‡æœ¬éƒ¨åˆ†åˆ—è¡¨
        content_type: å†…å®¹ç±»å‹
        active_rpd_versions: æ´»è·ƒçš„RPDç‰ˆæœ¬åˆ—è¡¨
        ai_review_orm: AIå®¡æŸ¥ORMå¯¹è±¡
        mode: å®¡æŸ¥æ¨¡å¼
        local_excel_path: æœ¬åœ°Excelæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        original_s3_path: åŸå§‹S3è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        extra_info: ç‰¹æ®Šè§„åˆ™åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºç©ºåˆ—è¡¨
                   æ ¼å¼: [{"speaker": "è§’è‰²A", "target": "è§’è‰²B", "alias": "ç‰¹æ®Šç§°å‘¼", "conditions": ["æ¡ä»¶1", "æ¡ä»¶2"]}]
    """
    start_time = time.time()
    model_name = QUALITY_MODEL if mode == AiReviewMode.QUALITY else SPEED_MODEL
    print(f"å¼€å§‹å¹¶è¡Œå¤„ç† {len(text_parts)} ä¸ªæ–‡æœ¬éƒ¨åˆ†")
    print(f"ä½¿ç”¨æ¨¡å‹: {model_name}")

    # ã€ä¼˜åŒ–1ã€‘æå‰è¿‡æ»¤text_reviewç±»å‹çš„RPDï¼Œé¿å…æ¯ä¸ªä»»åŠ¡é‡å¤è¿‡æ»¤
    text_rpds = [
        v for v in active_rpd_versions if v.parent_key == 'text_review']
    if not text_rpds:
        print("æœªæ‰¾åˆ° text_review ç±»å‹çš„RPDï¼Œè·³è¿‡æ–‡æœ¬å¤„ç†ã€‚")
        return []

    # ã€ä¼˜åŒ–2ã€‘ä»text_review RPDçš„reference_filesä¸­åŠ è½½ç§°å‘¼è¡¨
    alias_dict = _load_appellation_from_rpds(text_rpds)
    if not alias_dict:
        print("æœªèƒ½ä»ä»»ä½•text_review RPDä¸­åŠ è½½ç§°å‘¼è¡¨ï¼Œè·³è¿‡æ–‡æœ¬å¤„ç†ã€‚")
        return []

    # è®¾ç½®å¹¶å‘ä¿¡å·é‡
    if max_concurrent is None:
        max_concurrent = MAX_CONCURRENT
    semaphore = asyncio.Semaphore(max_concurrent)

    tasks = []
    for i, text_part_data in enumerate(text_parts):
        prepared_data = {
            "original_index": text_part_data.get("original_index", 0),
            "speaker": text_part_data.get("speaker", ""),
            "line": text_part_data.get("line", ""),
            "sheet_index": text_part_data.get("sheet_index", 0)
        }

        task = asyncio.create_task(
            _process_single_text_part(
                prepared_data,           # ä¼ é€’å‡†å¤‡å¥½çš„æ•°æ®ï¼Œè€Œä¸æ˜¯åŸå§‹æ•°æ®
                content_type,
                text_rpds,              # ä¼ é€’è¿‡æ»¤åçš„RPDåˆ—è¡¨
                ai_review_orm,
                alias_dict,             # ä¼ é€’é¢„åŠ è½½çš„å‘¼ç§°è¡¨
                semaphore,
                model_name,
                extra_info              # ä¼ é€’ç‰¹æ®Šè§„åˆ™
            ),
            name=f"text_part_task_{i}"
        )
        tasks.append(task)

    print(f"æ‰€æœ‰ {len(tasks)} ä¸ªæ–‡æœ¬éƒ¨åˆ†ä»»åŠ¡å·²åˆ›å»ºï¼Œå¼€å§‹ç­‰å¾…å®Œæˆ...")

    # ä½¿ç”¨å¯ä¸­æ–­çš„ç­‰å¾…æœºåˆ¶
    done = await _wait_for_tasks_with_cancellation(tasks, ai_review_orm, 2.0)

    # å¦‚æœè¿”å›ç©ºåˆ—è¡¨ï¼Œè¯´æ˜è¢«ä¸­æ–­äº†
    if not done:
        return []

    # æ”¶é›†æ‰€æœ‰ç»“æœ
    all_findings = []
    for task in done:
        try:
            findings = await task
            all_findings.extend(findings)
        except Exception as e:
            print(f"è·å–æ–‡æœ¬éƒ¨åˆ†å¤„ç†ç»“æœæ—¶å‡ºé”™: {e}")

    end_time = time.time()
    print(f"æ–‡æœ¬éƒ¨åˆ†å¹¶è¡Œå¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
    print(f"å…±ç”Ÿæˆ {len(all_findings)} ä¸ªfindings")

    # ã€æ–°å¢åŠŸèƒ½ã€‘å¦‚æœæä¾›äº†Excelæ–‡ä»¶è·¯å¾„ï¼Œåˆ™æ›´æ–°Excelæ–‡ä»¶å¹¶ä¸Šä¼ åˆ°S3
    if local_excel_path and original_s3_path and all_findings:
        try:
            print("å¼€å§‹æ›´æ–°Excelæ–‡ä»¶...")
            updated_s3_path = await _update_excel_with_findings(
                local_excel_path, text_parts, all_findings, original_s3_path, extra_info, text_rpds
            )
            print(f"Excelæ–‡ä»¶å·²æ›´æ–°å¹¶æ›¿æ¢åŸæ–‡ä»¶: {updated_s3_path}")

            # æ¸…ç†æœ¬åœ°ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(local_excel_path):
                os.unlink(local_excel_path)
                print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {local_excel_path}")

        except Exception as e:
            print(f"æ›´æ–°Excelæ–‡ä»¶å¤±è´¥: {e}")
            # å³ä½¿Excelæ›´æ–°å¤±è´¥ï¼Œä¹Ÿè¦æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if local_excel_path and os.path.exists(local_excel_path):
                os.unlink(local_excel_path)
    elif local_excel_path:
        # å¦‚æœæ²¡æœ‰findingsæˆ–è€…å‚æ•°ä¸å®Œæ•´ï¼Œä¹Ÿè¦æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            if os.path.exists(local_excel_path):
                os.unlink(local_excel_path)
                print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {local_excel_path}")
        except Exception as e:
            print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

    return all_findings


def _analyze_target_used_names_status(
    check_speaker_result: Dict[str, Any],
    alias_dict: Dict[str, Any],
    extra_info: List[Dict[str, Any]] = None
) -> List[Dict[str, Any]]:
    """
    åˆ†ææ¯ä¸ªtarget_used_nameçš„çŠ¶æ€ï¼Œè¿”å›è¯¦ç»†çš„çŠ¶æ€ä¿¡æ¯

    Args:
        check_speaker_result: check_speakerçš„ç»“æœ
        alias_dict: å‘¼ç§°è¡¨
        extra_info: ç‰¹æ®Šè§„åˆ™åˆ—è¡¨

    Returns:
        List[Dict]: æ¯ä¸ªtarget_used_nameçš„çŠ¶æ€ä¿¡æ¯
        [{"used_name": str, "status": "correct|error|special|unknown_speaker|unknown_target", "target": str}, ...]
    """
    if extra_info is None:
        extra_info = []

    results = []
    # æ³¨æ„ï¼šæ ¹æ®check_speakerçš„å®é™…è¿”å›ç»“æ„ï¼Œä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
    target_list = check_speaker_result.get(
        "target_list", [])  # æˆ‘ä»¬å­˜å‚¨æ—¶ç”¨çš„æ˜¯target_list
    target_used_names = check_speaker_result.get("target_used_names", [])
    speaker_name = check_speaker_result.get("detected_speaker", "")

    # ã€æ–°å¢ã€‘ä¸€è‡´æ€§æ£€æŸ¥ï¼šå¦‚æœspeakerä¸åœ¨ç§°å‘¼è¡¨ä¸­ï¼Œæ ‡è®°æ‰€æœ‰target_used_namesä¸ºunknown_speaker
    if speaker_name not in alias_dict:
        for i, target in enumerate(target_list):
            if i >= len(target_used_names):
                continue
            used_name = target_used_names[i]
            results.append({
                "used_name": used_name,
                "status": "unknown_speaker",
                "target": target,
                "reason": f"Speaker '{speaker_name}' not found in alias dictionary"
            })
        return results

    # è·å–è¯¥è¯´è¯è€…çš„ç§°å‘¼è§„åˆ™
    speaker_alias_rules = alias_dict.get(speaker_name, {})

    for i, target in enumerate(target_list):
        if i >= len(target_used_names):
            continue

        used_name = target_used_names[i]

        # ã€æ–°å¢ã€‘æ£€æŸ¥targetæ˜¯å¦åœ¨ç§°å‘¼è¡¨çš„æ‰€æœ‰è§’è‰²åˆ—è¡¨ä¸­
        all_characters_in_dict = set()
        for speaker_targets in alias_dict.values():
            all_characters_in_dict.update(speaker_targets.keys())
        all_characters_in_dict.update(alias_dict.keys())  # ä¹ŸåŒ…æ‹¬æ‰€æœ‰speaker

        if target not in all_characters_in_dict:
            results.append({
                "used_name": used_name,
                "status": "unknown_target",
                "target": target,
                "reason": f"Target '{target}' not found in alias dictionary"
            })
            continue

        # 1. å¸¸è§„è§„åˆ™æ£€æŸ¥
        correct_alias = speaker_alias_rules.get(target)
        if correct_alias and (used_name == correct_alias or used_name == target):
            results.append({
                "used_name": used_name,
                "status": "correct",
                "target": target
            })
            continue

        # 2. ç‰¹æ®Šè§„åˆ™æ£€æŸ¥
        is_special_case = False
        if extra_info:
            for rule in extra_info:
                if (rule.get('speaker') == speaker_name and
                    rule.get('target') == target and
                        rule.get('alias') == used_name):
                    results.append({
                        "used_name": used_name,
                        "status": "special",
                        "target": target,
                        "conditions": rule.get('conditions', ['ç„¡ç‰¹å®šæ¡ä»¶'])
                    })
                    is_special_case = True
                    break

        if is_special_case:
            continue

        # 3. é”™è¯¯æƒ…å†µ
        results.append({
            "used_name": used_name,
            "status": "error",
            "target": target
        })

    return results


def _create_formatted_text_with_highlights(
    original_text: str,
    target_status_list: List[Dict[str, Any]]
) -> Any:
    """
    åˆ›å»ºå¸¦æœ‰æ ¼å¼åŒ–é«˜äº®çš„æ–‡æœ¬å¯¹è±¡

    Args:
        original_text: åŸå§‹æ–‡æœ¬
        target_status_list: target_used_nameçš„çŠ¶æ€åˆ—è¡¨

    Returns:
        openpyxl.styles.RichTextå¯¹è±¡
    """
    # å®šä¹‰é¢œè‰²
    GREEN = "00AA00"    # æ­£ç¡® - ç»¿è‰²
    RED = "FF0000"      # é”™è¯¯ - çº¢è‰²
    ORANGE = "FF8800"   # ç‰¹æ®Šè§„åˆ™ - æ©™è‰²
    PURPLE = "8800AA"   # æœªçŸ¥speaker/target - ç´«è‰²

    # åˆ›å»ºå¯Œæ–‡æœ¬å—åˆ—è¡¨
    text_blocks = []
    current_pos = 0

    # æŒ‰ç…§used_nameåœ¨æ–‡æœ¬ä¸­çš„å‡ºç°ä½ç½®æ’åº
    name_positions = []
    for target_status in target_status_list:
        used_name = target_status["used_name"]
        pos = 0
        while True:
            pos = original_text.find(used_name, pos)
            if pos == -1:
                break
            name_positions.append({
                "start": pos,
                "end": pos + len(used_name),
                "status": target_status["status"],
                "used_name": used_name
            })
            pos += len(used_name)

    # æŒ‰ä½ç½®æ’åº
    name_positions.sort(key=lambda x: x["start"])

    # åˆ›å»ºæ™®é€šå­—ä½“ï¼ˆç”¨äºéæ ¼å¼åŒ–æ–‡æœ¬ï¼‰
    normal_font = InlineFont()

    # æ„å»ºå¯Œæ–‡æœ¬å—
    for name_pos in name_positions:
        # æ·»åŠ å‰é¢çš„æ™®é€šæ–‡æœ¬
        if current_pos < name_pos["start"]:
            normal_text = original_text[current_pos:name_pos["start"]]
            if normal_text:
                text_blocks.append(
                    TextBlock(text=normal_text, font=normal_font))

        # æ·»åŠ æ ¼å¼åŒ–çš„target_used_name
        status = name_pos["status"]
        if status == "correct":
            color = GREEN
        elif status == "error":
            color = RED
        elif status == "special":
            color = ORANGE
        elif status in ["unknown_speaker", "unknown_target"]:
            color = PURPLE
        else:
            color = None

        if color:
            # ä½¿ç”¨InlineFontè€Œä¸æ˜¯Font
            formatted_font = InlineFont(b=True, color=color)
            text_blocks.append(TextBlock(
                text=name_pos["used_name"],
                font=formatted_font
            ))
        else:
            # æ™®é€šç§°å‘¼ä¹Ÿéœ€è¦å­—ä½“
            text_blocks.append(
                TextBlock(text=name_pos["used_name"], font=normal_font))

        current_pos = name_pos["end"]

    # æ·»åŠ å‰©ä½™çš„æ™®é€šæ–‡æœ¬
    if current_pos < len(original_text):
        remaining_text = original_text[current_pos:]
        if remaining_text:
            text_blocks.append(
                TextBlock(text=remaining_text, font=normal_font))

    # åˆ›å»ºå¯Œæ–‡æœ¬å¯¹è±¡
    if text_blocks:
        return CellRichText(text_blocks)
    else:
        return original_text


async def _update_excel_with_findings(
    excel_file_path: str,
    text_parts: List[Dict[str, Any]],
    findings_results: List[AiReviewFindingEntry],
    s3_path: str,
    extra_info: List[Dict[str, Any]] = None,
    text_rpds: List[ReviewPointDefinitionVersionInDB] = None
) -> str:
    """
    å°†AIå®¡æŸ¥ç»“æœæ›´æ–°åˆ°Excelæ–‡ä»¶å¹¶ä¸Šä¼ åˆ°S3

    Args:
        excel_file_path: æœ¬åœ°Excelæ–‡ä»¶è·¯å¾„
        text_parts: åŸå§‹æ–‡æœ¬éƒ¨åˆ†æ•°æ®
        findings_results: AIå®¡æŸ¥ç»“æœ
        s3_path: åŸå§‹S3è·¯å¾„
        extra_info: ç‰¹æ®Šè§„åˆ™åˆ—è¡¨ï¼Œç”¨äºExcelæ ¼å¼åŒ–æ—¶æ£€æµ‹ç‰¹æ®Šè§„åˆ™

    Returns:
        str: æ›´æ–°åçš„S3è·¯å¾„
    """
    try:
        # ä»text_review RPDçš„reference_filesä¸­åŠ è½½ç§°å‘¼è¡¨
        alias_dict = {}
        if text_rpds:
            alias_dict = _load_appellation_from_rpds(text_rpds)

        if not alias_dict:
            print("æœªèƒ½ä»RPDä¸­åŠ è½½ç§°å‘¼è¡¨ï¼Œä½¿ç”¨ç©ºå­—å…¸")
            alias_dict = {}

        # æ„å»ºfindingsæ˜ å°„è¡¨ï¼ŒæŒ‰(sheet_index, original_index)åˆ†ç»„
        findings_map = {}
        for finding in findings_results:
            if finding.content_metadata:
                sheet_idx = finding.content_metadata.get('sheet_index', 0)
                orig_idx = finding.content_metadata.get('original_index', 0)
                key = (sheet_idx, orig_idx)

                if key not in findings_map:
                    findings_map[key] = []
                findings_map[key].append({
                    'description': finding.description,
                    'severity': finding.severity,
                    'tag': finding.tag,
                    'content_metadata': finding.content_metadata  # ä¿å­˜å®Œæ•´çš„content_metadata
                })

        # ä½¿ç”¨openpyxlåŠ è½½Excelæ–‡ä»¶
        workbook = load_workbook(excel_file_path)

        # ä¸_download_and_process_textä¿æŒä¸€è‡´çš„åˆ—å
        text_col = "æœ¬æ–‡ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–"

        # æŒ‰å·¥ä½œè¡¨å¤„ç†
        sheet_names = workbook.sheetnames
        for sheet_index, sheet_name in enumerate(sheet_names):
            if sheet_name not in workbook.sheetnames:
                continue

            worksheet = workbook[sheet_name]

            # è¯»å–è¯¥å·¥ä½œè¡¨çš„pandas DataFrameæ¥è·å–åˆ—ä½ç½®
            try:
                df = pd.read_excel(
                    excel_file_path, sheet_name=sheet_name, skiprows=11, engine='openpyxl')
                if text_col not in df.columns:
                    print(f"å·¥ä½œè¡¨ {sheet_name} ä¸­æœªæ‰¾åˆ°æ–‡æœ¬åˆ—ï¼Œè·³è¿‡")
                    continue

                text_col_idx = df.columns.get_loc(text_col)
                result_col_idx = text_col_idx + 2  # åœ¨æ–‡æœ¬åˆ—å³ä¾§ç¬¬2åˆ—å†™å…¥ç»“æœ

            except Exception as e:
                print(f"è¯»å–å·¥ä½œè¡¨ {sheet_name} å¤±è´¥: {e}")
                continue

            # éå†è¯¥å·¥ä½œè¡¨çš„æ‰€æœ‰æ–‡æœ¬éƒ¨åˆ†
            for text_part in text_parts:
                if text_part.get('sheet_index') != sheet_index:
                    continue

                original_index = text_part.get('original_index', 0)
                key = (sheet_index, original_index)

                # ã€æ–°å¢åŠŸèƒ½ã€‘æ ¼å¼åŒ–åŸå§‹æ–‡æœ¬åˆ—ï¼Œæ ‡è®°target_used_name
                row_num = original_index + 13
                original_text = text_part.get('line', '')

                if key in findings_map:
                    findings = findings_map[key]

                    # ä»findingsä¸­æå–check_speakerç»“æœ
                    check_speaker_result = None
                    if key in findings_map and findings_map[key]:
                        # è·å–content_metadataä¸­çš„check_speakerç»“æœ
                        for finding in findings:
                            if finding.get('content_metadata'):
                                check_speaker_result = finding['content_metadata'].get(
                                    'check_speaker_result')
                                if check_speaker_result:
                                    break

                    if check_speaker_result and check_speaker_result.get('target_used_names'):
                        try:
                            # åˆ†ætarget_used_namesçš„çŠ¶æ€
                            target_status_list = _analyze_target_used_names_status(
                                check_speaker_result,
                                alias_dict,
                                extra_info=extra_info or []  # ä¼ å…¥å®é™…çš„ç‰¹æ®Šè§„åˆ™
                            )

                            # åˆ›å»ºæ ¼å¼åŒ–çš„æ–‡æœ¬
                            if target_status_list and original_text:
                                formatted_text = _create_formatted_text_with_highlights(
                                    original_text,
                                    target_status_list
                                )

                                # æ›´æ–°æ–‡æœ¬åˆ—çš„å†…å®¹
                                # pandasçš„åˆ—ç´¢å¼•ä»0å¼€å§‹ï¼Œopenpyxlçš„åˆ—ç´¢å¼•ä»1å¼€å§‹ï¼Œæ‰€ä»¥éœ€è¦+1
                                text_cell = worksheet.cell(
                                    row=row_num, column=text_col_idx + 1)

                                if hasattr(formatted_text, '__dict__'):
                                    print(
                                        f"  - formatted_textè¯¦æƒ…: {formatted_text.__dict__}")

                                text_cell.value = formatted_text

                                print(
                                    f"âœ… æ ¼å¼åŒ–æ–‡æœ¬å®Œæˆ (sheet: {sheet_name}, row: {row_num}): æ ‡è®°äº† {len(target_status_list)} ä¸ªç§°å‘¼")
                            else:
                                print(
                                    f"âš ï¸ è·³è¿‡æ ¼å¼åŒ–ï¼štarget_status_listä¸ºç©ºæˆ–original_textä¸ºç©º")
                        except Exception as format_error:
                            print(
                                f"âŒ æ ¼å¼åŒ–æ–‡æœ¬å¤±è´¥ (sheet: {sheet_name}, row: {row_num}): {format_error}")
                            import traceback
                            traceback.print_exc()
                            # æ ¼å¼åŒ–å¤±è´¥æ—¶ï¼Œä¿æŒåŸå§‹æ–‡æœ¬ä¸å˜
                    else:
                        print(f"âš ï¸ è·³è¿‡æ ¼å¼åŒ–ï¼šæ²¡æœ‰check_speakerç»“æœæˆ–target_used_namesä¸ºç©º")

                    # æ„å»ºæ£€æŸ¥ç»“æœæ¶ˆæ¯
                    # check_messages = []
                    risk_severity_messages = []
                    alert_severity_messages = []
                    safe_severity_messages = []

                    for finding in findings:
                        severity = finding.get('severity', 'alert')
                        description = finding.get('description', '')
                        # tag = finding.get('tag', '')

                        if severity == 'risk':
                            risk_severity_messages.append(
                                f"ã€ã‚¨ãƒ©ãƒ¼ã€‘{description}")
                        elif severity == 'alert':
                            alert_severity_messages.append(
                                f"ã€è­¦å‘Šã€‘{description}")
                        elif severity == 'safe':
                            # å¯¹äºsafeæƒ…å†µï¼Œåªæ˜¾ç¤ºã€OKã€‘å•é¡Œãªã—ï¼Œä¸æ˜¾ç¤ºã‚»ãƒªãƒ•å†…å®¹
                            safe_severity_messages.append("ã€OKã€‘å•é¡Œãªã—")
                        else:
                            # å‘åå…¼å®¹æ—§çš„severityå€¼
                            if severity in ['high', 'medium', 'low']:
                                if severity == 'high':
                                    risk_severity_messages.append(
                                        f"ã€ã‚¨ãƒ©ãƒ¼ã€‘{description}")
                                elif severity in ['medium', 'low']:
                                    alert_severity_messages.append(
                                        f"ã€è­¦å‘Šã€‘{description}")
                            else:
                                # æœªçŸ¥severityé»˜è®¤ä¸ºalert
                                alert_severity_messages.append(
                                    f"ã€è­¦å‘Šã€‘{description}")

                    # ç»„åˆæ¶ˆæ¯ï¼šriské”™è¯¯ä¼˜å…ˆï¼Œç„¶åæ˜¯alertè­¦å‘Šï¼›è‹¥å‡æ— ä¸”å­˜åœ¨safeï¼Œåˆ™è¾“å‡ºsafe
                    all_messages = risk_severity_messages + alert_severity_messages
                    if not all_messages and safe_severity_messages:
                        all_messages = safe_severity_messages
                    check_message = "\n".join(
                        all_messages) if all_messages else ""

                    if check_message or key in findings_map:
                        # å†™å…¥Excelå•å…ƒæ ¼ï¼ˆè¡Œå·éœ€è¦+12å› ä¸ºè·³è¿‡äº†11è¡Œï¼‰
                        try:
                            # +1åˆ—ï¼šå†™å…¥detected_speaker
                            speaker_info = ""
                            if check_speaker_result:
                                detected_speaker = check_speaker_result.get(
                                    'detected_speaker', '')
                                if detected_speaker:
                                    speaker_info = detected_speaker

                            # +2åˆ—ï¼šå†™å…¥target_list
                            target_info = []
                            if check_speaker_result:
                                target_list = check_speaker_result.get(
                                    'target_list', [])
                                if target_list:
                                    target_info = target_list

                            # å†™å…¥æ•°æ®åˆ°ä¸åŒåˆ—
                            if speaker_info:
                                worksheet.cell(
                                    row=row_num, column=result_col_idx + 1, value=speaker_info)
                            if target_info:
                                # target_infoæ˜¯listï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²å†™å…¥Excel
                                target_str = ", ".join(target_info) if isinstance(
                                    target_info, list) else str(target_info)
                                worksheet.cell(
                                    row=row_num, column=result_col_idx + 2, value=target_str)
                            if check_message:
                                worksheet.cell(
                                    row=row_num, column=result_col_idx + 3, value=check_message)

                            # print(f"æ›´æ–°å·¥ä½œè¡¨ {sheet_name}, è¡Œ {row_num}: speaker={speaker_info}, targets={target_info}, message={check_message[:30]}...")
                        except Exception as e:
                            print(
                                f"å†™å…¥Excelå•å…ƒæ ¼å¤±è´¥ (sheet: {sheet_name}, row: {row_num}): {e}")

        # ä¿å­˜Excelæ–‡ä»¶
        # print(f"ğŸ“ å¼€å§‹ä¿å­˜Excelæ–‡ä»¶: {excel_file_path}")
        workbook.save(excel_file_path)
        print(f"âœ… Excelæ–‡ä»¶å·²æ›´æ–°å¹¶ä¿å­˜: {excel_file_path}")

        try:
            # é‡æ–°åŠ è½½éªŒè¯
            verify_workbook = load_workbook(excel_file_path)
            print(
                f"ğŸ“‹ éªŒè¯ï¼šé‡æ–°åŠ è½½Excelæ–‡ä»¶æˆåŠŸï¼Œå·¥ä½œè¡¨æ•°é‡: {len(verify_workbook.sheetnames)}")
            verify_workbook.close()
        except Exception as verify_error:
            print(f"âš ï¸ éªŒè¯Excelæ–‡ä»¶æ—¶å‡ºé”™: {verify_error}")

        # ä¸Šä¼ åˆ°S3
        new_s3_path = await _upload_updated_excel_to_s3(excel_file_path, s3_path)
        return new_s3_path

    except Exception as e:
        print(f"æ›´æ–°Excelæ–‡ä»¶å¤±è´¥: {e}")
        raise e


async def _upload_updated_excel_to_s3(local_file_path: str, original_s3_path: str) -> str:
    """
    å°†æ›´æ–°åçš„Excelæ–‡ä»¶ä¸Šä¼ åˆ°S3ï¼Œç›´æ¥æ›¿æ¢åŸæ–‡ä»¶

    Args:
        local_file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
        original_s3_path: åŸå§‹S3è·¯å¾„ï¼ˆæ”¯æŒå®Œæ•´S3 URLæˆ–ç›¸å¯¹è·¯å¾„ï¼‰

    Returns:
        str: S3è·¯å¾„ï¼ˆä¸åŸæ–‡ä»¶è·¯å¾„ç›¸åŒï¼‰
    """
    try:
        from aimage_supervision.settings import AWS_BUCKET_NAME

        # å¤„ç†S3è·¯å¾„ï¼šæ”¯æŒå®Œæ•´URLå’Œç›¸å¯¹è·¯å¾„
        if original_s3_path.startswith("s3://"):
            # å®Œæ•´S3 URLæ ¼å¼ï¼šs3://bucket/path/file.xlsx
            s3_parts = original_s3_path.replace("s3://", "").split("/", 1)
            bucket_name = s3_parts[0]
            object_key = s3_parts[1] if len(s3_parts) > 1 else ""
        else:
            # ç›¸å¯¹è·¯å¾„æ ¼å¼ï¼špath/file.xlsx
            bucket_name = AWS_BUCKET_NAME
            object_key = original_s3_path

        # ç›´æ¥ä½¿ç”¨åŸæ–‡ä»¶åï¼ˆæ›¿æ¢ç­–ç•¥ï¼‰
        new_object_key = object_key  # ç›´æ¥ä½¿ç”¨åŸæ–‡ä»¶çš„object_keyï¼Œå®ç°æ–‡ä»¶æ›¿æ¢

        # ä¸Šä¼ åˆ°S3ï¼ˆä½¿ç”¨ä¸å…¶ä»–å‡½æ•°ç›¸åŒçš„é…ç½®ï¼‰
        from aimage_supervision.settings import (AWS_ACCESS_KEY_ID, AWS_REGION,
                                                 AWS_SECRET_ACCESS_KEY)

        s3_client = boto3.client(
            's3',
            region_name=AWS_REGION,
            aws_access_key_id=AWS_ACCESS_KEY_ID,
            aws_secret_access_key=AWS_SECRET_ACCESS_KEY
        )
        # æ£€æŸ¥åŸæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        try:
            s3_client.head_object(Bucket=bucket_name, Key=new_object_key)
            print(f"ğŸ”„ å°†ç›´æ¥æ›¿æ¢åŸæ–‡ä»¶: {new_object_key}")
        except s3_client.exceptions.NoSuchKey:
            print(f"âš ï¸ åŸæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶: {new_object_key}")
        except Exception as e:
            print(f"æ£€æŸ¥åŸæ–‡ä»¶å­˜åœ¨æ€§æ—¶å‡ºé”™: {e}")

        # ä¸Šä¼ æ–‡ä»¶ï¼ˆç›´æ¥æ›¿æ¢åŸæ–‡ä»¶ï¼‰
        s3_client.upload_file(local_file_path, bucket_name, new_object_key)

        new_s3_path = f"s3://{bucket_name}/{new_object_key}"
        print(f"âœ… Excelæ–‡ä»¶å·²æˆåŠŸæ›¿æ¢åŸæ–‡ä»¶åˆ°S3: {new_s3_path}")

        return new_s3_path

    except Exception as e:
        print(f"ä¸Šä¼ Excelæ–‡ä»¶åˆ°S3å¤±è´¥: {e}")
        raise e


async def _build_final_ai_review_data(
    ai_review_orm: AiReview,
    finding_entries: List[AiReviewFindingEntry]
) -> AiReviewInDB:
    """æ„å»ºæœ€ç»ˆçš„AIå®¡æŸ¥æ•°æ®å¹¶æ›´æ–°æ•°æ®åº“"""
    finding_entry_schemas = [
        AiReviewFindingEntrySchema.from_orm(fe) for fe in finding_entries]

    # æ„å»ºæ£€æµ‹å…ƒç´ çš„æ‘˜è¦æ•°æ®
    detected_elements_summary: Dict[str, Any] = {
        "description": f"AIå®¡æŸ¥å‘ç°äº† {len(finding_entries)} ä¸ªé—®é¢˜",
        "elements": []
    }

    # æ ¹æ®findingsç”Ÿæˆæ£€æµ‹å…ƒç´ æ‘˜è¦
    for finding_entry in finding_entries:
        # ä½¿ç”¨tagä½œä¸ºnameï¼Œå¦‚æœæ²¡æœ‰tagåˆ™ä½¿ç”¨æˆªæ–­çš„description
        element_name = finding_entry.tag if finding_entry.tag else finding_entry.description[
            :50]

        # ç¡®ä¿ç¬¦åˆAiDetectedElement schemaçš„å¿…éœ€å­—æ®µ
        element = {
            "name": element_name,
            "confidence": 0.8,
            "label": "object",  # å¿…éœ€å­—æ®µï¼šä½¿ç”¨'object'ä½œä¸ºé»˜è®¤label
            # å¿…éœ€å­—æ®µï¼šä½¿ç”¨findingçš„areaæˆ–é»˜è®¤å€¼
            "area": finding_entry.area or {"x": 0, "y": 0, "width": 0, "height": 0},
            "character_id": None
        }
        detected_elements_summary["elements"].append(element)

    # æ›´æ–°æ•°æ®åº“ä¸­çš„ai_review_ormå¯¹è±¡
    ai_review_orm.ai_review_output_json = detected_elements_summary
    ai_review_orm.review_timestamp = ai_review_orm.created_at
    await ai_review_orm.save()

    final_review_data = AiReviewInDB(
        id=ai_review_orm.id,
        created_at=ai_review_orm.created_at,
        updated_at=ai_review_orm.updated_at,
        subtask_id=ai_review_orm.subtask.id,
        version=ai_review_orm.version,
        is_latest=ai_review_orm.is_latest,
        review_timestamp=ai_review_orm.review_timestamp,
        initiated_by_user_id=ai_review_orm.initiated_by_user.id if ai_review_orm.initiated_by_user else None,
        last_modified_by_user_id=ai_review_orm.last_modified_by_user.id if ai_review_orm.last_modified_by_user else None,
        findings=finding_entry_schemas
    )

    return final_review_data


async def get_subtasks_for_tasks(task_ids: List[UUID]) -> List[Subtask]:
    """
    Retrieves all subtask objects associated with a list of task IDs.
    """
    subtasks = await Subtask.filter(task_id__in=task_ids).all()
    return subtasks


async def initiate_ai_review_for_subtask(
    subtask_id: UUID,
    initiated_by_user_id: UUID,
    cr_check: Optional[bool] = False,
    rpd_ids: Optional[List[UUID]] = None,
    mode: AiReviewMode = AiReviewMode.QUALITY
) -> AiReviewInDB:
    """
    ä¸ºæŒ‡å®šçš„å­ä»»åŠ¡å¯åŠ¨å®Œæ•´çš„AIå®¡æŸ¥æµç¨‹ã€‚
    è¿™åŒ…æ‹¬:
    1. è·å–å­ä»»åŠ¡æ•°æ®å’Œå›¾åƒ
    2. å¯¹å›¾åƒæ‰§è¡Œå…ƒç´ æ£€æµ‹
    3. åˆ›å»ºæ–°çš„AiReviewè®°å½•ï¼ˆç‰ˆæœ¬åŒ–ï¼‰
    4. ä¸ºæ¯ä¸ªæ´»è·ƒçš„ReviewPointDefinitionVersionç”Ÿæˆå’Œå­˜å‚¨findings
    5. è¿”å›ç»¼åˆçš„AiReviewæ•°æ®
    """
    ai_review_orm = None
    try:
        # 1. åˆ›å»ºAI Reviewè®°å½• - çŠ¶æ€ï¼šPENDING
        ai_review_orm = await _create_new_ai_review_version(subtask_id, initiated_by_user_id)

        # 2. æ›´æ–°çŠ¶æ€ä¸ºPROCESSING
        await _update_processing_status(
            ai_review_orm,
            AiReviewProcessingStatus.PROCESSING
        )

        # 3. æ‰§è¡ŒAIå¤„ç†ï¼ˆåŸºäºå†…å®¹ç±»å‹çš„åˆ†æ”¯å¤„ç†ï¼‰
        subtask = await _validate_and_fetch_subtask(subtask_id)

        # æ£€æŸ¥subtaskçš„å†…å®¹ç±»å‹
        content_type = subtask.content.task_type if subtask.content else SubtaskType.PICTURE

        active_rpd_versions = await get_active_review_point_versions(
            project_id=subtask.task.project.id,
            cr_check=cr_check,
            rpd_ids=rpd_ids
        )

        if not active_rpd_versions:
            print(
                f"Warning: No active/specified Review Point Definition Versions found for project {subtask.task.project.id}. "
                f"AI Review for subtask {subtask_id} will have no findings from RPDs."
            )

        if content_type == SubtaskType.PICTURE:
            # å›¾ç‰‡å¤„ç†é€»è¾‘ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if await _check_cancellation_signal(ai_review_orm):
                await _update_processing_status(ai_review_orm, AiReviewProcessingStatus.CANCELLED)
                return await _build_final_ai_review_data(ai_review_orm, [])

            image_bytes, image_width, image_height = await _download_and_process_image(
                subtask_id, subtask.content.s3_path
            )
            content_metadata: Dict[str, Any] = {}
            all_finding_entries = await _generate_findings_for_all_rpd_versions(
                image_bytes, image_width, image_height, content_type, content_metadata, active_rpd_versions, ai_review_orm, mode,
            )

        elif content_type == SubtaskType.VIDEO:
            # è§†é¢‘å¤„ç†é€»è¾‘ - å¹¶è¡Œå¤„ç†æ‰€æœ‰è§†é¢‘å¸§
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if await _check_cancellation_signal(ai_review_orm):
                await _update_processing_status(ai_review_orm, AiReviewProcessingStatus.CANCELLED)
                return await _build_final_ai_review_data(ai_review_orm, [])

            print(f"Processing video content for subtask {subtask_id}")
            video_data, video_width, video_height = await _download_and_process_video(subtask_id, subtask.content.s3_path)

            # ä½¿ç”¨å¹¶è¡Œå¤„ç†å‡½æ•°å¤„ç†æ‰€æœ‰è§†é¢‘å¸§
            all_finding_entries = await _generate_findings_for_video_frames_parallel(
                video_data, video_width, video_height, content_type,
                active_rpd_versions, ai_review_orm, mode
            )

        elif content_type in [SubtaskType.TEXT, SubtaskType.WORD, SubtaskType.EXCEL]:
            # æ–‡æœ¬/æ–‡æ¡£å¤„ç†é€»è¾‘ - å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡æœ¬éƒ¨åˆ†
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if await _check_cancellation_signal(ai_review_orm):
                await _update_processing_status(ai_review_orm, AiReviewProcessingStatus.CANCELLED)
                return await _build_final_ai_review_data(ai_review_orm, [])

            print(
                f"Processing {content_type.value} content for subtask {subtask_id}")

            try:
                text_parts, local_excel_path = await _download_and_process_text(subtask_id, subtask.content.s3_path)
                if text_parts:
                    # ä½¿ç”¨å¹¶è¡Œå¤„ç†å‡½æ•°å¤„ç†æ‰€æœ‰æ–‡æœ¬éƒ¨åˆ†ï¼Œå¹¶æ›´æ–°Excelæ–‡ä»¶
                    # ä»text_review RPDçš„special_ruleså­—æ®µè·å–ç‰¹æ®Šè§„åˆ™
                    extra_info = []
                    text_rpds = [
                        v for v in active_rpd_versions if v.parent_key == 'text_review']
                    for rpd in text_rpds:
                        if rpd.special_rules and isinstance(rpd.special_rules, list):
                            # special_rulesç°åœ¨æ˜¯JSONæ ¼å¼çš„åˆ—è¡¨
                            for rule in rpd.special_rules:
                                if isinstance(rule, dict) and all(key in rule for key in ['speaker', 'target', 'alias']):
                                    extra_info.append({
                                        "speaker": rule.get('speaker', ''),
                                        "target": rule.get('target', ''),
                                        "alias": rule.get('alias', ''),
                                        "conditions": rule.get('conditions', []),
                                        "rpd_title": rpd.title
                                    })
                    print(f"ä»RPDè·å–åˆ° {len(extra_info)} æ¡ç‰¹æ®Šè§„åˆ™")

                    all_finding_entries = await _generate_findings_for_text_parts_parallel(
                        text_parts, content_type, active_rpd_versions, ai_review_orm, mode,
                        local_excel_path, subtask.content.s3_path, extra_info=extra_info)
                else:
                    print(f"Error: {text_parts}")
                    all_finding_entries = []
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if 'local_excel_path' in locals() and os.path.exists(local_excel_path):
                        os.unlink(local_excel_path)

            except NotImplementedError:
                print(f"Error: {NotImplementedError}")
                all_finding_entries = []

        elif content_type == SubtaskType.AUDIO:
            # éŸ³é¢‘å¤„ç†é€»è¾‘ - å¹¶è¡Œå¤„ç†æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if await _check_cancellation_signal(ai_review_orm):
                await _update_processing_status(ai_review_orm, AiReviewProcessingStatus.CANCELLED)
                return await _build_final_ai_review_data(ai_review_orm, [])

            print(f"Processing audio content for subtask {subtask_id}")

            try:
                # TODO: å®ç°éŸ³é¢‘ä¸‹è½½å’Œåˆ†å‰²é€»è¾‘
                # audio_data, audio_metadata = await _download_and_process_audio(subtask_id, subtask.content.s3_path)
                # audio_segments = await _split_audio_into_segments(audio_data, audio_metadata)

                # æš‚æ—¶åˆ›å»ºä¸€ä¸ªç¤ºä¾‹éŸ³é¢‘ç‰‡æ®µåˆ—è¡¨ï¼Œå±•ç¤ºå¹¶è¡Œå¤„ç†çš„ç”¨æ³•
                # å®é™…å®ç°æ—¶ï¼Œaudio_segmentsåº”è¯¥æ¥è‡ªäºéŸ³é¢‘åˆ†å‰²å‡½æ•°
                audio_segments = []  # ç©ºåˆ—è¡¨ï¼Œå› ä¸ºéŸ³é¢‘å¤„ç†åŠŸèƒ½å°šæœªå®ç°

                if audio_segments:
                    ...
                    # ä½¿ç”¨å¹¶è¡Œå¤„ç†å‡½æ•°å¤„ç†æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
                    # all_finding_entries = await _generate_findings_for_audio_segments_parallel(
                    #     audio_segments, content_type, active_rpd_versions, ai_review_orm, mode, max_concurrent=5
                    # )
                else:
                    print(f"éŸ³é¢‘å¤„ç†åŠŸèƒ½å°šæœªå®Œæ•´å®ç°ï¼Œè·³è¿‡éŸ³é¢‘å†…å®¹")
                    all_finding_entries = []  # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œç›´åˆ°å®ç°éŸ³é¢‘å¤„ç†é€»è¾‘

            except NotImplementedError:
                print(f"éŸ³é¢‘å¤„ç†åŠŸèƒ½å°šæœªå®ç°ï¼Œè·³è¿‡éŸ³é¢‘å†…å®¹")
                all_finding_entries = []

        else:
            # æœªçŸ¥ç±»å‹å¤„ç†
            print(
                f"Unknown content type {content_type} for subtask {subtask_id}, skipping AI review")
            all_finding_entries = []

        # 4. æ„å»ºæœ€ç»ˆæ•°æ®
        final_data = await _build_final_ai_review_data(ai_review_orm, all_finding_entries)

        # 5. æ›´æ–°çŠ¶æ€ä¸ºCOMPLETED
        await _update_processing_status(
            ai_review_orm,
            AiReviewProcessingStatus.COMPLETED
        )

        return final_data

    except Exception as e:
        # 6. å¼‚å¸¸å¤„ç†ï¼šæ›´æ–°çŠ¶æ€ä¸ºFAILEDå¹¶å›é€€åˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬
        if ai_review_orm:
            await _update_processing_status(
                ai_review_orm,
                AiReviewProcessingStatus.FAILED,
                error_message=str(e)
            )

            # æ‰§è¡Œå›é€€é€»è¾‘
            logger.info(
                f"AI review failed for subtask {subtask_id}, attempting rollback...")
            # ç¡®ä¿subtaskå…³ç³»å·²åŠ è½½
            if not hasattr(ai_review_orm, 'subtask') or ai_review_orm.subtask is None:
                await ai_review_orm.fetch_related('subtask')
            await _rollback_failed_review(ai_review_orm)

        raise


# --- Service functions for retrieving AI Review data (Phase 3) ---

async def _map_ai_review_orm_to_schema(review_orm: AiReview) -> AiReviewSchema:
    """
    Helper function to map an AiReview ORM model to the AiReviewSchema Pydantic model,
    populating findings and detected_elements.
    """
    # Ensure findings and related objects are loaded
    await review_orm.fetch_related('findings', 'subtask', 'initiated_by_user', 'last_modified_by_user')

    # finding_schemas will be List[AiReviewFindingEntryInDB]
    finding_schemas_in_db = [AiReviewFindingEntryInDB.from_orm(
        f) for f in review_orm.findings]

    # Debug: Check what's in the ORM objects and schema objects
    # for f in review_orm.findings:
    #     print(f"DEBUG ORM Finding {f.id}: is_fixed = {f.is_fixed}")

    # for f_in_db in finding_schemas_in_db:
    #     print(
    #         f"DEBUG Schema Finding {f_in_db.id}: is_fixed = {f_in_db.is_fixed}")
    #     print(f"DEBUG model_dump: {f_in_db.model_dump()}")

    # Convert to List[AiReviewFindingEntry] as expected by AiReviewSchema

    final_findings_list: List[AiReviewFindingEntrySchema] = [
        AiReviewFindingEntrySchema(**f_in_db.model_dump()) for f_in_db in finding_schemas_in_db
    ]

    # Debug: Check final findings list
    # for final_finding in final_findings_list:
    #     print(
    #         f"DEBUG Final Finding {final_finding.id}: is_fixed = {final_finding.is_fixed}")
    #     print(f"DEBUG Final Finding model_dump: {final_finding.model_dump()}")

    parsed_summary_object: Optional[AiDetectedElements] = None
    if review_orm.ai_review_output_json and isinstance(review_orm.ai_review_output_json, dict):
        try:
            parsed_summary_object = AiDetectedElements.model_validate(
                review_orm.ai_review_output_json)
        except Exception as e:
            print(
                f"Warning: Could not parse ai_review_output_json for AiReview ID {review_orm.id}: {e}")
            # parsed_summary_object remains None if validation fails

    final_detected_elements_list: Optional[List[AiDetectedElement]] = None
    if parsed_summary_object:
        final_detected_elements_list = parsed_summary_object.elements

    subtask_id_val = review_orm.subtask.id
    initiated_by_user_id_val = review_orm.initiated_by_user.id if review_orm.initiated_by_user else None
    last_modified_by_user_id_val = review_orm.last_modified_by_user.id if review_orm.last_modified_by_user else None

    return AiReviewSchema(
        id=review_orm.id,
        subtask_id=subtask_id_val,
        version=review_orm.version,
        is_latest=review_orm.is_latest,
        review_timestamp=review_orm.review_timestamp,
        initiated_by_user_id=initiated_by_user_id_val,
        last_modified_by_user_id=last_modified_by_user_id_val,
        created_at=review_orm.created_at,
        updated_at=review_orm.updated_at,
        findings=final_findings_list,
        detected_elements=final_detected_elements_list,
        detected_elements_summary=parsed_summary_object,
        # æ–°å¢çŠ¶æ€å­—æ®µ
        processing_status=review_orm.processing_status.value if review_orm.processing_status else None,
        error_message=review_orm.error_message,
        processing_started_at=review_orm.processing_started_at,
        processing_completed_at=review_orm.processing_completed_at
    )


async def get_ai_review_by_id(ai_review_id: UUID) -> Optional[AiReviewSchema]:
    """
    Retrieves a specific AI Review by its ID, including its findings and parsed detected elements.
    """
    try:
        review_orm = await AiReview.get_or_none(id=ai_review_id)
        if not review_orm:
            return None
        return await _map_ai_review_orm_to_schema(review_orm)
    except DoesNotExist:  # Should be caught by get_or_none, but as a safeguard
        return None


async def get_latest_ai_review_for_subtask(subtask_id: UUID) -> Optional[AiReviewSchema]:
    """
    Retrieves the latest AI Review for a given subtask, including findings and parsed detected elements.
    """
    review_orm = await AiReview.filter(subtask_id=subtask_id, is_latest=True).first()
    if not review_orm:
        return None
    return await _map_ai_review_orm_to_schema(review_orm)


async def list_ai_reviews_for_subtask(subtask_id: UUID) -> List[AiReviewSchema]:
    """
    List all AI reviews for a subtask, ordered by version descending.
    """
    review_orms = await AiReview.filter(subtask_id=subtask_id).order_by('-version').prefetch_related('findings')
    review_schemas = []
    for review_orm in review_orms:
        review_schemas.append(await _map_ai_review_orm_to_schema(review_orm))
    return review_schemas


class CharacterPredictionResponse(BaseModel):
    character_count: int
    character_name: List[str]


async def predict_character_for_subtask(
    subtask: Subtask, character_candidates: List[Character],
) -> Tuple[List[Character] | None, float]:
    """é¢„æµ‹å­ä»»åŠ¡ä¸­çš„è§’è‰²

    Args:
        subtask: å­ä»»åŠ¡å¯¹è±¡
        character_candidates: å€™é€‰è§’è‰²åˆ—è¡¨
    """

    # ä¸ºè¿™ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„ Gemini å®¢æˆ·ç«¯
    task_gemini_client = genai.Client(api_key=os.getenv(
        'GEMINI_API_KEY')) if os.getenv('GEMINI_API_KEY') else None
    if not task_gemini_client:
        raise ValueError("GEMINI_API_KEY not set. Cannot predict character.")

    # æ£€æŸ¥è§’è‰²å€™é€‰åˆ—è¡¨ä¸­æ˜¯å¦æœ‰æœ‰æ•ˆçš„å›¾ç‰‡è·¯å¾„
    valid_characters = []
    for character in character_candidates:
        if not character.image_path or character.image_path.strip() == "":
            print(
                f"Warning: Character {character.name} (ID: {character.id}) has no image_path, skipping...")
            continue
        valid_characters.append(character)

    if not valid_characters:
        raise ValueError(
            "No characters with valid image paths found in the project")

    character_image_prompts = []
    for character in valid_characters:
        try:
            image_bytes = download_file_content_from_s3_sync(
                character.image_path)
            character_image_prompts.extend([
                f"Character: {character.name}",
                types.Part.from_bytes(data=image_bytes, mime_type='image/jpeg')
            ])
        except Exception as e:
            print(
                f"Error downloading image for character {character.name} (path: {character.image_path}): {e}")
            continue

    if not character_image_prompts:
        raise ValueError("Failed to download images for any characters")

    # get subtask image from s3
    subtask_image_path = subtask.content.s3_path
    if not subtask_image_path or subtask_image_path.strip() == "":
        raise ValueError("Subtask has no valid s3_path for content")

    subtask_image_bytes = download_file_content_from_s3_sync(
        subtask_image_path)

    # ä½¿ç”¨æ–°çš„å·¥å…·å‡½æ•°è‡ªåŠ¨å¤„ç†å›¾ç‰‡æˆ–è§†é¢‘ï¼Œå¦‚æœæ˜¯è§†é¢‘ä¼šè‡ªåŠ¨æå–ç¬¬ä¸€å¸§
    subtask_image_bytes = process_image_or_video_bytes(subtask_image_bytes)

    model_name = "gemini-2.5-flash"
    contents = ["The following is a list of characters and their images: "] + \
        character_image_prompts + \
        ["Please predict the characters in the following image",
         types.Part.from_bytes(
             data=subtask_image_bytes, mime_type='image/jpeg'),
         "Return the number of characters in the image and the names of the characters in a list.",
         "The subtask content is: " + str(subtask.content)]
    response = task_gemini_client.models.generate_content(
        model=model_name,
        contents=contents,
        config=genai.types.GenerateContentConfig(
            response_mime_type='application/json',
            response_schema=CharacterPredictionResponse,
            system_instruction="You are a character analysis AI. Recognize how many characters are in the image and return the names of the characters.",
            temperature=0.0,
        ),
    )

    character_count = response.parsed.character_count
    character_names = response.parsed.character_name
    print(
        f"Character count: {character_count}, Character names: {character_names}")

    if character_count != len(character_names):
        raise ValueError(
            f"Character count {character_count} does not match the number of character names {len(character_names)}")

    predicted_characters = []
    for character_name in character_names:
        for character in valid_characters:
            # print(f"Character name: {character_name}")
            character_name = character_name.strip()
            if character.name.lower() == character_name.lower():
                predicted_characters.append(character)
    print(f"Predicted characters: {predicted_characters}")
    if len(predicted_characters) == 0:
        return None, 1.0

    return predicted_characters, 1.0


async def _validate_and_fetch_subtask(subtask_id: UUID) -> Subtask:
    """éªŒè¯å¹¶è·å–subtaskæ•°æ®"""
    try:
        subtask = await Subtask.get(id=subtask_id).prefetch_related('task__project')
    except DoesNotExist:
        raise ValueError(f"Subtask with ID {subtask_id} not found.")

    if not subtask.content or not subtask.content.s3_path:
        raise ValueError(
            f"Subtask {subtask_id} does not have a valid s3_path in its content.")

    return subtask


async def _download_and_process_image(subtask_id: UUID, s3_path: str) -> tuple[bytes, int, int]:
    """ä¸‹è½½å¹¶å¤„ç†å›¾åƒï¼Œè¿”å›å›¾åƒå­—èŠ‚æ•°æ®å’Œå°ºå¯¸"""
    image_path_local = None

    try:
        image_path_local = await download_image_from_s3(s3_path)
        with open(image_path_local, 'rb') as f:
            image_bytes = f.read()

        image = Image.open(image_path_local)
        image_width, image_height = image.size

        return image_bytes, image_width, image_height

    except Exception as e:
        # Clean up if download or read failed
        if image_path_local and os.path.exists(image_path_local):
            os.remove(image_path_local)
        raise ValueError(
            f"Failed to download or read image for subtask {subtask_id} from {s3_path}: {e}")


async def _download_and_process_video(subtask_id: UUID, s3_path: str) -> Tuple[List[Dict[str, Any]], int, int]:
    """ä¸‹è½½å¹¶å¤„ç†è§†é¢‘ï¼Œè¿”å›è§†é¢‘å­—èŠ‚æ•°æ®å’Œå°ºå¯¸"""
    video_path_local = None

    try:
        video_path_local = await download_video_from_s3(s3_path)
        video_frames_info = detect_video_scenes_and_extract_frames(
            video_path_local)
        video_width, video_height = video_frames_info["width"], video_frames_info["height"]
        return video_frames_info["scenes"], video_width, video_height

    except Exception as e:
        # Clean up if download or read failed
        if video_path_local and os.path.exists(video_path_local):
            os.remove(video_path_local)
        raise ValueError(
            f"Failed to download or read video for subtask {subtask_id} from {s3_path}: {e}")


async def _check_cancellation_signal(ai_review_orm: AiReview) -> bool:
    """æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­ä¿¡å·"""
    await ai_review_orm.refresh_from_db(fields=['should_cancel'])
    if ai_review_orm.should_cancel:
        logger.info(
            f"Cancellation signal detected for AI review {ai_review_orm.id}")
    return ai_review_orm.should_cancel


async def _rollback_failed_review(failed_review_orm: AiReview) -> None:
    """
    å›é€€å¤±è´¥çš„AI Reviewåˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬

    Args:
        failed_review_orm: å¤±è´¥çš„AI Review ORMå¯¹è±¡
    """
    try:
        # ç¡®ä¿subtaskå…³ç³»å·²åŠ è½½
        await failed_review_orm.fetch_related('subtask')
        subtask_id = failed_review_orm.subtask.id
        failed_version = failed_review_orm.version

        logger.info(
            f"Rolling back failed AI review version {failed_version} for subtask {subtask_id}")

        # æŸ¥æ‰¾ä¸Šä¸€ä¸ªæˆåŠŸçš„ç‰ˆæœ¬
        logger.info(
            f"Searching for successful versions before version {failed_version}")
        all_previous_reviews = await AiReview.filter(
            subtask=failed_review_orm.subtask,
            version__lt=failed_version
        ).order_by('-version')

        logger.info(f"Found {len(all_previous_reviews)} previous versions:")
        for review in all_previous_reviews:
            logger.info(
                f"  Version {review.version}: status={review.processing_status}, is_latest={review.is_latest}")

        previous_successful_review = await AiReview.filter(
            subtask=failed_review_orm.subtask,
            version__lt=failed_version,
            processing_status=AiReviewProcessingStatus.COMPLETED.value
        ).order_by('-version').first()

        if previous_successful_review:
            # æœ‰ä¸Šä¸€ä¸ªæˆåŠŸç‰ˆæœ¬ï¼Œå›é€€åˆ°è¯¥ç‰ˆæœ¬
            logger.info(
                f"Found previous successful version {previous_successful_review.version}, rolling back to it")

            # å°†å¤±è´¥ç‰ˆæœ¬çš„is_latestè®¾ç½®ä¸ºFalse
            failed_review_orm.is_latest = False
            await failed_review_orm.save(update_fields=['is_latest'])

            # å°†ä¸Šä¸€ä¸ªæˆåŠŸç‰ˆæœ¬è®¾ç½®ä¸ºlatest
            previous_successful_review.is_latest = True
            await previous_successful_review.save(update_fields=['is_latest'])

            logger.info(
                f"Successfully rolled back to version {previous_successful_review.version}")

        else:
            # æ²¡æœ‰ä¸Šä¸€ä¸ªæˆåŠŸç‰ˆæœ¬ï¼Œå›é€€åˆ°"æ— ç›‘ä¿®çŠ¶æ€"
            logger.info(
                "No previous successful version found, rolling back to no-review state")

            # å°†å¤±è´¥ç‰ˆæœ¬çš„is_latestè®¾ç½®ä¸ºFalseï¼Œè¿™æ ·get_latest_ai_review_for_subtaskä¼šè¿”å›None
            failed_review_orm.is_latest = False
            await failed_review_orm.save(update_fields=['is_latest'])

            logger.info("Successfully rolled back to no-review state")

    except Exception as rollback_error:
        logger.error(
            f"Failed to rollback AI review {failed_review_orm.id}: {rollback_error}")
        import traceback
        logger.error(f"Rollback traceback: {traceback.format_exc()}")
        # å›é€€å¤±è´¥ä¸åº”è¯¥å½±å“ä¸»è¦çš„é”™è¯¯å¤„ç†æµç¨‹ï¼Œä½†æˆ‘ä»¬éœ€è¦è®°å½•è¯¦ç»†é”™è¯¯
        pass


async def _wait_for_tasks_with_cancellation(
    tasks: List[asyncio.Task],
    ai_review_orm: AiReview,
    check_interval: float = 2.0
) -> List[asyncio.Task]:
    """
    å¯ä¸­æ–­çš„ä»»åŠ¡ç­‰å¾…æœºåˆ¶
    æ¯éš”check_intervalç§’æ£€æŸ¥ä¸€æ¬¡ä¸­æ–­ä¿¡å·ï¼Œå¦‚æœæ£€æµ‹åˆ°ä¸­æ–­åˆ™å–æ¶ˆæ‰€æœ‰ä»»åŠ¡
    è¿”å›å·²å®Œæˆçš„ä»»åŠ¡åˆ—è¡¨
    """
    done = set()
    pending = set(tasks)

    while pending:
        # æ£€æŸ¥ä¸­æ–­ä¿¡å·
        if await _check_cancellation_signal(ai_review_orm):
            print("æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œå–æ¶ˆæ‰€æœ‰å¾…å¤„ç†ä»»åŠ¡")
            # å–æ¶ˆæ‰€æœ‰å¾…å¤„ç†çš„ä»»åŠ¡
            for task in pending:
                task.cancel()
            # ç­‰å¾…å·²å–æ¶ˆçš„ä»»åŠ¡å®Œæˆæ¸…ç†
            await asyncio.gather(*pending, return_exceptions=True)
            print("æ‰€æœ‰ä»»åŠ¡å·²å–æ¶ˆ")
            return []

        # ç­‰å¾…è‡³å°‘ä¸€ä¸ªä»»åŠ¡å®Œæˆï¼Œæˆ–è€…è¶…æ—¶æ£€æŸ¥ä¸­æ–­ä¿¡å·
        completed, pending = await asyncio.wait(
            pending,
            return_when=asyncio.FIRST_COMPLETED,
            timeout=check_interval
        )

        done.update(completed)

        if completed:
            print(f"å·²å®Œæˆ {len(done)} / {len(tasks)} ä¸ªä»»åŠ¡")

    return list(done)


async def _update_processing_status(
    ai_review_orm: AiReview,
    status: AiReviewProcessingStatus,
    error_message: Optional[str] = None
) -> None:
    """æ›´æ–°AI Reviewå¤„ç†çŠ¶æ€çš„ç»Ÿä¸€å‡½æ•°"""
    ai_review_orm.processing_status = status

    if status == AiReviewProcessingStatus.PROCESSING:
        ai_review_orm.processing_started_at = datetime.now(timezone.utc)
    elif status in [AiReviewProcessingStatus.COMPLETED, AiReviewProcessingStatus.FAILED, AiReviewProcessingStatus.CANCELLED]:
        ai_review_orm.processing_completed_at = datetime.now(timezone.utc)
        if error_message:
            ai_review_orm.error_message = error_message

    await ai_review_orm.save()


async def _create_new_ai_review_version(subtask_id: UUID, initiated_by_user_id: UUID) -> AiReview:
    """åˆ›å»ºæ–°çš„AIå®¡æŸ¥ç‰ˆæœ¬ï¼ŒçŠ¶æ€é»˜è®¤ä¸ºPENDING"""
    # è·å–subtaskå’Œuserå¯¹è±¡
    subtask = await Subtask.get(id=subtask_id)
    user = await User.get(id=initiated_by_user_id)

    previous_reviews = await AiReview.filter(subtask=subtask).order_by("-version")

    # ç‰ˆæœ¬ç®¡ç†ç­–ç•¥ï¼šä¿ç•™å†å²ç‰ˆæœ¬ï¼Œåªæ›´æ–°is_latestæ ‡è®°
    if previous_reviews:
        print(f"æ‰¾åˆ° {len(previous_reviews)} ä¸ªæ—§çš„AI Reviewè®°å½•ï¼Œå°†æ›´æ–°is_latestæ ‡è®°...")
        # å°†æ‰€æœ‰æ—§ç‰ˆæœ¬çš„is_latestè®¾ç½®ä¸ºFalse
        await AiReview.filter(subtask=subtask).update(is_latest=False)

        # è®¡ç®—æ–°ç‰ˆæœ¬å·
        latest_version = previous_reviews[0].version
        new_review_version = latest_version + 1
        print(f"åˆ›å»ºæ–°ç‰ˆæœ¬: {new_review_version}")
    else:
        # é¦–æ¬¡åˆ›å»ºï¼Œç‰ˆæœ¬å·ä¸º1
        new_review_version = 1
        print("é¦–æ¬¡åˆ›å»ºAI Reviewï¼Œç‰ˆæœ¬å·ä¸º1")

    ai_review_orm = await AiReview.create(
        subtask=subtask,
        version=new_review_version,
        is_latest=True,
        ai_review_output_json={},
        initiated_by_user=user,
        processing_status=AiReviewProcessingStatus.PENDING  # æ˜ç¡®è®¾ç½®åˆå§‹çŠ¶æ€
    )

    return ai_review_orm


async def get_relevant_findings_for_subtask(subtask_id: UUID) -> List[AiReviewFindingEntry]:
    """
    è·å–å­ä»»åŠ¡çš„ç›¸å…³å‘ç°ï¼šåŒ…æ‹¬æœ€æ–°ç‰ˆæœ¬çš„æ‰€æœ‰å‘ç° + å†å²ç‰ˆæœ¬ä¸­è¢«æ ‡è®°ä¸º is_fixed çš„å‘ç°
    è¿™ç§æ–¹æ³•é¿å…äº†éå†æ‰€æœ‰å†å²ç‰ˆæœ¬ï¼Œåªé€šè¿‡ä¸€æ¬¡æŸ¥è¯¢è·å–æ‰€æœ‰ç›¸å…³æ•°æ®
    """
    # ä½¿ç”¨å•ä¸ªæŸ¥è¯¢è·å–æ‰€æœ‰ç›¸å…³çš„å‘ç°æ¡ç›®
    # æ¡ä»¶ï¼šå±äºè¯¥ subtask ä¸” (æ˜¯æœ€æ–°ç‰ˆæœ¬ OR è¢«æ ‡è®°ä¸º is_fixed)
    findings = await AiReviewFindingEntry.filter(
        ai_review__subtask_id=subtask_id
    ).filter(
        Q(ai_review__is_latest=True) | Q(is_fixed=True)
    ).prefetch_related(
        'ai_review',
        'review_point_definition_version',
        'review_point_definition_version__review_point_definition'
    ).order_by('-ai_review__version', 'created_at')

    return findings


async def get_findings_summary_for_subtask(subtask_id: UUID) -> Optional[AiReviewSchema]:
    """
    è·å–å­ä»»åŠ¡çš„å‘ç°æ‘˜è¦ï¼Œè¿”å›ä¸ get_latest_ai_review_for_subtask ç›¸åŒçš„æ ¼å¼ï¼Œ
    ä½† findings å­—æ®µåŒ…å«ä¼˜åŒ–è¿‡çš„æ•°æ®ï¼šæœ€æ–°ç‰ˆæœ¬çš„findings + å†å²ç‰ˆæœ¬ä¸­æ ‡è®°ä¸º is_fixed çš„findings
    è¿™é¿å…äº†æ‰«ææ‰€æœ‰å†å²ç‰ˆæœ¬ï¼Œæä¾›æ›´å¥½çš„æ€§èƒ½ã€‚
    """
    # è·å–æœ€æ–°çš„ AI review ORM å¯¹è±¡
    latest_review_orm = await AiReview.filter(subtask_id=subtask_id, is_latest=True).first()
    if not latest_review_orm:
        return None

    # è·å–ç›¸å…³çš„ findingsï¼ˆæœ€æ–°ç‰ˆæœ¬ + å†å²ç‰ˆæœ¬ä¸­ is_fixed çš„ï¼‰
    relevant_findings = await get_relevant_findings_for_subtask(subtask_id)

    # ç¡®ä¿ç›¸å…³å¯¹è±¡å·²åŠ è½½
    await latest_review_orm.fetch_related('subtask', 'initiated_by_user', 'last_modified_by_user')

    # å°†ç›¸å…³ findings è½¬æ¢ä¸º schema æ ¼å¼
    finding_schemas_in_db = [
        AiReviewFindingEntryInDB.from_orm(f) for f in relevant_findings]

    final_findings_list: List[AiReviewFindingEntrySchema] = [
        AiReviewFindingEntrySchema(**f_in_db.model_dump()) for f_in_db in finding_schemas_in_db
    ]

    # è§£æ detected_elements_summary
    parsed_summary_object: Optional[AiDetectedElements] = None
    if latest_review_orm.ai_review_output_json and isinstance(latest_review_orm.ai_review_output_json, dict):
        try:
            parsed_summary_object = AiDetectedElements.model_validate(
                latest_review_orm.ai_review_output_json)
        except Exception as e:
            print(
                f"Warning: Could not parse ai_review_output_json for AiReview ID {latest_review_orm.id}: {e}")

    final_detected_elements_list: Optional[List[AiDetectedElement]] = None
    if parsed_summary_object:
        final_detected_elements_list = parsed_summary_object.elements

    subtask_id_val = latest_review_orm.subtask.id
    initiated_by_user_id_val = latest_review_orm.initiated_by_user.id if latest_review_orm.initiated_by_user else None
    last_modified_by_user_id_val = latest_review_orm.last_modified_by_user.id if latest_review_orm.last_modified_by_user else None

    return AiReviewSchema(
        id=latest_review_orm.id,
        subtask_id=subtask_id_val,
        version=latest_review_orm.version,
        is_latest=latest_review_orm.is_latest,
        review_timestamp=latest_review_orm.review_timestamp,
        initiated_by_user_id=initiated_by_user_id_val,
        last_modified_by_user_id=last_modified_by_user_id_val,
        created_at=latest_review_orm.created_at,
        updated_at=latest_review_orm.updated_at,
        findings=final_findings_list,  # ä½¿ç”¨ä¼˜åŒ–è¿‡çš„ findings
        detected_elements=final_detected_elements_list,
        detected_elements_summary=parsed_summary_object,
        # æ–°å¢çŠ¶æ€å­—æ®µ
        processing_status=latest_review_orm.processing_status.value if latest_review_orm.processing_status else None,
        error_message=latest_review_orm.error_message,
        processing_started_at=latest_review_orm.processing_started_at,
        processing_completed_at=latest_review_orm.processing_completed_at
    )


async def update_finding_fixed_status(finding_id: UUID, is_fixed: bool) -> AiReviewFindingEntry:
    """
    æ›´æ–°å‘ç°æ¡ç›®çš„ is_fixed çŠ¶æ€
    """
    finding = await AiReviewFindingEntry.get_or_none(id=finding_id)
    if not finding:
        raise ValueError(f"Finding with ID {finding_id} not found")

    finding.is_fixed = is_fixed
    await finding.save()

    return finding


async def update_finding_content(
    finding_id: UUID,
    description: Optional[str] = None,
    severity: Optional[str] = None,
    suggestion: Optional[str] = None
) -> AiReviewFindingEntry:
    """
    æ›´æ–°AIå®¡æŸ¥å‘ç°æ¡ç›®çš„å†…å®¹

    Args:
        finding_id: å‘ç°æ¡ç›®çš„ID
        description: æ–°çš„æè¿°ï¼ˆå¯é€‰ï¼‰
        severity: æ–°çš„ä¸¥é‡ç¨‹åº¦ï¼ˆå¯é€‰ï¼‰
        suggestion: æ–°çš„å»ºè®®ï¼ˆå¯é€‰ï¼‰

    Returns:
        AiReviewFindingEntry: æ›´æ–°åçš„å‘ç°æ¡ç›®

    Raises:
        ValueError: å¦‚æœæ‰¾ä¸åˆ°å‘ç°æ¡ç›®
    """
    finding = await AiReviewFindingEntry.get_or_none(id=finding_id)
    if not finding:
        raise ValueError("Finding not found")

    # Update only provided fields
    if description is not None:
        finding.description = description
    if severity is not None:
        finding.severity = severity
    if suggestion is not None:
        finding.suggestion = suggestion

    # Update the updated_at timestamp
    finding.updated_at = datetime.now(timezone.utc)

    await finding.save()
    return finding


async def update_finding_bounding_box(
    finding_id: UUID,
    area: dict
) -> AiReviewFindingEntry:
    """
    æ›´æ–°AIå®¡æŸ¥å‘ç°æ¡ç›®çš„è¾¹ç•Œæ¡†

    Args:
        finding_id: å‘ç°æ¡ç›®çš„ID
        area: æ–°çš„è¾¹ç•Œæ¡†æ•°æ®ï¼ˆåŒ…å«x, y, width, heightï¼‰

    Returns:
        AiReviewFindingEntry: æ›´æ–°åçš„å‘ç°æ¡ç›®

    Raises:
        ValueError: å¦‚æœæ‰¾ä¸åˆ°å‘ç°æ¡ç›®
    """
    finding = await AiReviewFindingEntry.get_or_none(id=finding_id)
    if not finding:
        raise ValueError("Finding not found")

    # Update the area field
    finding.area = area

    # Update the updated_at timestamp
    finding.updated_at = datetime.now(timezone.utc)

    await finding.save()
    return finding


# --- Direct Image Analysis for Testing (New Function) ---

async def analyze_image_with_rpds(
    image_bytes: bytes,
    image_filename: str,
    rpd_ids: Optional[List[UUID]] = None,
    cr_check: bool = False,
    mode: AiReviewMode = AiReviewMode.QUALITY,
    user_id: UUID = None,
    project_id: Optional[UUID] = None
) -> Dict[str, Any]:
    """
    ç›´æ¥åˆ†æå›¾ç‰‡ä¸æŒ‡å®šçš„RPDsï¼Œä¸åˆ›å»ºæ•°æ®åº“è®°å½•

    è¿™æ˜¯ä¸“ä¸ºRPDæµ‹è¯•å’Œå¼€å‘è®¾è®¡çš„æ–¹æ³•ï¼Œç‰¹ç‚¹ï¼š
    - ç›´æ¥å¤„ç†å›¾ç‰‡å­—èŠ‚æ•°æ®
    - ä¸åˆ›å»ºAI Reviewæˆ–subtaskè®°å½•
    - å¿«é€Ÿè¿”å›åˆ†æç»“æœ
    - é€‚åˆæµ‹è¯•å’ŒåŸå‹å¼€å‘

    Args:
        image_bytes: å›¾ç‰‡çš„å­—èŠ‚æ•°æ®
        image_filename: å›¾ç‰‡æ–‡ä»¶åï¼ˆç”¨äºæ—¥å¿—ï¼‰
        rpd_ids: è¦ä½¿ç”¨çš„RPD IDåˆ—è¡¨
        cr_check: æ˜¯å¦å¯ç”¨ç‰ˆæƒæ£€æŸ¥
        mode: åˆ†ææ¨¡å¼ï¼ˆquality/speedï¼‰
        user_id: ç”¨æˆ·IDï¼ˆç”¨äºæ—¥å¿—ï¼‰
        project_id: é¡¹ç›®IDï¼ˆç”¨äºç­›é€‰RPDï¼‰

    Returns:
        DictåŒ…å«åˆ†æç»“æœï¼š
        {
            "success": True/False,
            "findings": [...],
            "rpd_count": int,
            "processing_time": float,
            "image_info": {...}
        }
    """
    start_time = time.time()

    try:
        # 1. å¤„ç†å’ŒéªŒè¯å›¾ç‰‡
        print(f"å¼€å§‹åˆ†æå›¾ç‰‡: {image_filename}")

        # è·å–å›¾ç‰‡å°ºå¯¸
        image_width, image_height = await _get_image_dimensions(image_bytes)

        # 2. è·å–æ´»è·ƒçš„RPDç‰ˆæœ¬
        active_rpd_versions = await get_active_review_point_versions(
            project_id=project_id,
            cr_check=cr_check,
            rpd_ids=rpd_ids
        )

        if not active_rpd_versions:
            return {
                "success": True,
                "findings": [],
                "rpd_count": 0,
                "processing_time": time.time() - start_time,
                "image_info": {
                    "filename": image_filename,
                    "width": image_width,
                    "height": image_height
                },
                "message": "æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„RPDç‰ˆæœ¬è¿›è¡Œåˆ†æ"
            }

        print(f"æ‰¾åˆ° {len(active_rpd_versions)} ä¸ªæ´»è·ƒçš„RPDç‰ˆæœ¬")

        # 3. å¹¶è¡Œç”Ÿæˆfindingsï¼ˆä¸ä¿å­˜åˆ°æ•°æ®åº“ï¼‰
        findings_data = await _generate_findings_for_image_analysis(
            image_bytes=image_bytes,
            image_width=image_width,
            image_height=image_height,
            active_rpd_versions=active_rpd_versions,
            mode=mode
        )

        # 4. æ ¼å¼åŒ–ç»“æœ
        formatted_findings = []
        for finding_data in findings_data:
            rpd_version = finding_data['rpd_version']
            finding_dict = finding_data['finding_dict']

            formatted_finding = {
                "rpd_id": str(rpd_version.review_point_definition_id),
                "rpd_key": rpd_version.parent_key,
                "rpd_title": rpd_version.title,
                "description": finding_dict['description'],
                "severity": finding_dict['severity'],
                "suggestion": finding_dict.get('suggestion', ''),
                "tag": finding_dict.get('tag', ''),
                "confidence": finding_dict.get('confidence', 0.8),  # é»˜è®¤ç½®ä¿¡åº¦
            }

            # æ·»åŠ è¾¹ç•Œæ¡†ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'bounding_boxes' in finding_data:
                formatted_finding['area'] = finding_data['bounding_boxes']

            formatted_findings.append(formatted_finding)

        processing_time = time.time() - start_time

        result = {
            "success": True,
            "findings": formatted_findings,
            "rpd_count": len(active_rpd_versions),
            "processing_time": processing_time,
            "image_info": {
                "filename": image_filename,
                "width": image_width,
                "height": image_height
            },
            "message": f"æˆåŠŸåˆ†æå›¾ç‰‡ï¼Œå‘ç° {len(formatted_findings)} ä¸ªé—®é¢˜ç‚¹"
        }

        print(f"å›¾ç‰‡åˆ†æå®Œæˆ: {image_filename}, å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        return result

    except Exception as e:
        processing_time = time.time() - start_time
        error_message = f"å›¾ç‰‡åˆ†æå¤±è´¥: {str(e)}"
        print(error_message)

        return {
            "success": False,
            "findings": [],
            "rpd_count": 0,
            "processing_time": processing_time,
            "image_info": {
                "filename": image_filename,
                "width": 0,
                "height": 0
            },
            "error": error_message
        }


async def _generate_findings_for_image_analysis(
    image_bytes: bytes,
    image_width: int,
    image_height: int,
    active_rpd_versions: List[ReviewPointDefinitionVersionInDB],
    mode: AiReviewMode,
    ai_review_orm: Optional[AiReview] = None
) -> List[Dict[str, Any]]:
    """
    ä¸ºå›¾ç‰‡åˆ†æç”Ÿæˆfindingsï¼Œä¸ä¿å­˜åˆ°æ•°æ®åº“
    è¿™æ˜¯_generate_findings_for_all_rpd_versionsçš„ç®€åŒ–ç‰ˆæœ¬
    """
    start_time = time.time()

    model_name = QUALITY_MODEL if mode == AiReviewMode.QUALITY else SPEED_MODEL
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {model_name} è¿›è¡Œå›¾ç‰‡åˆ†æ")

    # è®¾ç½®å¹¶å‘é™åˆ¶
    semaphore = asyncio.Semaphore(MAX_CONCURRENT)  # é€‚ä¸­çš„å¹¶å‘æ•°

    print(f"å¼€å§‹å¹¶è¡Œåˆ†æ {len(active_rpd_versions)} ä¸ªRPDç‰ˆæœ¬")

    # åˆ›å»ºä»»åŠ¡
    tasks = []
    for rpd_version in active_rpd_versions:
        task = asyncio.create_task(
            _generate_single_rpd_findings(
                image_bytes, image_width, image_height, rpd_version, semaphore, model_name
            ),
            name=f"rpd_analysis_{rpd_version.title}"
        )
        tasks.append(task)

    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆå¦‚æœæä¾›äº†ai_review_ormï¼Œä½¿ç”¨å¯ä¸­æ–­æœºåˆ¶ï¼‰
    if ai_review_orm:
        done = await _wait_for_tasks_with_cancellation(tasks, ai_review_orm, 2.0)
        if not done:
            return []
    else:
        done, pending = await asyncio.wait(tasks, return_when=asyncio.ALL_COMPLETED)

    parallel_time = time.time() - start_time
    print(f"å¹¶è¡Œåˆ†æå®Œæˆï¼Œè€—æ—¶: {parallel_time:.2f}ç§’")

    # æ”¶é›†ç»“æœ
    all_findings_data = []
    for task in done:
        try:
            findings_data_list = await task
            if not isinstance(findings_data_list, Exception):
                all_findings_data.extend(findings_data_list)
        except Exception as e:
            print(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
            continue

    return all_findings_data


async def _get_image_dimensions(image_bytes: bytes) -> Tuple[int, int]:
    """
    è·å–å›¾ç‰‡å°ºå¯¸
    """
    try:

        image = Image.open(io.BytesIO(image_bytes))
        return image.size  # (width, height)
    except Exception as e:
        print(f"è·å–å›¾ç‰‡å°ºå¯¸å¤±è´¥: {e}")
        return (1920, 1080)  # é»˜è®¤å°ºå¯¸


async def get_latest_rpd_execution_summary(subtask_id: UUID) -> Dict:
    """è·å–æœ€æ–°AIå®¡æ ¸çš„RPDæ‰§è¡Œæ‘˜è¦"""

    logger.info(f"æ­£åœ¨æŸ¥è¯¢subtask {subtask_id} çš„æœ€æ–°AIå®¡æ ¸è®°å½•...")

    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•AIå®¡æ ¸è®°å½•
    all_reviews = await AiReview.filter(subtask_id=subtask_id).all()
    logger.info(f"æ‰¾åˆ° {len(all_reviews)} ä¸ªAIå®¡æ ¸è®°å½•")

    if all_reviews:
        for review in all_reviews:
            logger.info(
                f"AIå®¡æ ¸è®°å½•: version={review.version}, is_latest={review.is_latest}, id={review.id}")

    # è·å–æœ€æ–°çš„AIå®¡æ ¸è®°å½•
    latest_review = await AiReview.filter(
        subtask_id=subtask_id,
        is_latest=True
    ).first()

    if latest_review:
        # æ‰‹åŠ¨åŠ è½½findingså’Œç›¸å…³æ•°æ®
        await latest_review.fetch_related('findings__review_point_definition_version__review_point_definition')
        logger.info(f"åŠ è½½findingsåï¼Œæ•°é‡: {len(latest_review.findings)}")

        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰findingsï¼Œå°è¯•ç›´æ¥æŸ¥è¯¢
        if len(latest_review.findings) == 0:
            direct_findings = await AiReviewFindingEntry.filter(
                ai_review_id=latest_review.id
            ).prefetch_related('review_point_definition_version__review_point_definition').all()
            logger.info(f"ç›´æ¥æŸ¥è¯¢findingsæ•°é‡: {len(direct_findings)}")

            # å¦‚æœç›´æ¥æŸ¥è¯¢æœ‰ç»“æœï¼Œä½¿ç”¨ç›´æ¥æŸ¥è¯¢çš„ç»“æœè¿›è¡Œå¤„ç†
            if direct_findings:
                findings_to_process = direct_findings
            else:
                findings_to_process = list(latest_review.findings)
        else:
            findings_to_process = list(latest_review.findings)

    if not latest_review:
        logger.info(f"æœªæ‰¾åˆ°subtask {subtask_id} çš„æœ€æ–°AIå®¡æ ¸è®°å½•")
        return {
            'ai_review_version': None,
            'executed_at': None,
            'executed_rpds': [],
            'total_findings': 0
        }

    logger.info(
        f"æ‰¾åˆ°æœ€æ–°AIå®¡æ ¸è®°å½•: version={latest_review.version}, findingsæ•°é‡={len(findings_to_process)}")

    # ç»Ÿè®¡æ‰§è¡Œçš„RPD
    rpd_summary: Dict[str, Dict[str, Any]] = {}
    logger.info(f"å¼€å§‹å¤„ç† {len(findings_to_process)} ä¸ªfindings...")

    for i, finding in enumerate(findings_to_process):
        logger.info(
            f"å¤„ç†finding {i+1}: id={finding.id}, description={finding.description[:50]}...")

        try:
            # æ£€æŸ¥æ˜¯å¦æ­£ç¡®é¢„åŠ è½½äº†å…³è”æ•°æ®
            rpd_version = finding.review_point_definition_version
            rpd = rpd_version.review_point_definition

            # ä½¿ç”¨RPD IDä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦ï¼Œè€Œä¸æ˜¯key
            rpd_id = str(rpd.id)
            rpd_key = rpd.key

            logger.info(
                f"  RPDä¿¡æ¯: id={rpd_id}, key={rpd_key}, title={rpd_version.title}, version={rpd_version.version_number}")

            if rpd_id not in rpd_summary:
                rpd_summary[rpd_id] = {
                    'rpd_key': rpd_key,
                    'rpd_title': rpd_version.title,
                    'version_number': rpd_version.version_number,
                    'finding_count': 0,
                    'severities': []
                }
            rpd_summary[rpd_id]['finding_count'] += 1
            rpd_summary[rpd_id]['severities'].append(finding.severity)

        except Exception as e:
            logger.error(f"  å¤„ç†findingæ—¶å‡ºé”™: {e}")

            traceback.print_exc()

    logger.info(f"RPDç»Ÿè®¡ç»“æœ: {list(rpd_summary.keys())}")
    logger.info(f"æ€»findingsæ•°: {len(findings_to_process)}")

    return {
        'ai_review_version': latest_review.version,
        'executed_at': latest_review.review_timestamp,
        'executed_rpds': list(rpd_summary.values()),
        'total_findings': len(findings_to_process)
    }


# --- Single RPD Version Data Analysis (New Function) ---

async def analyze_single_image_with_rpd_data(
    image_bytes: bytes,
    image_filename: str,
    rpd_version_data: Dict[str, Any],
    cr_check: bool = False,
    mode: AiReviewMode = AiReviewMode.QUALITY,
    user_id: Optional[UUID] = None
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ä¼ å…¥çš„RPD versionæ•°æ®ç›´æ¥åˆ†æå›¾ç‰‡ï¼Œä¸ä¾èµ–æ•°æ®åº“

    ä¸“ä¸ºRPDæµ‹è¯•è®¾è®¡ï¼Œç‰¹ç‚¹ï¼š
    - æ¥å—RPD versionçš„åŸå§‹æ•°æ®è€Œä¸æ˜¯æ•°æ®åº“ID
    - åˆ›å»ºä¸´æ—¶çš„RPD versionå¯¹è±¡ç”¨äºåˆ†æ
    - ä¸ä¿å­˜ä»»ä½•æ•°æ®åˆ°æ•°æ®åº“
    - å¿«é€Ÿè¿”å›åˆ†æç»“æœ

    Args:
        image_bytes: å›¾ç‰‡çš„å­—èŠ‚æ•°æ®
        image_filename: å›¾ç‰‡æ–‡ä»¶å
        rpd_version_data: RPD versionçš„æ•°æ®å­—å…¸ï¼ŒåŒ…å«title, parent_keyç­‰å­—æ®µ
        cr_check: æ˜¯å¦å¯ç”¨ç‰ˆæƒæ£€æŸ¥ï¼ˆå½“å‰ç‰ˆæœ¬å¿½ç•¥æ­¤å‚æ•°ï¼‰
        mode: åˆ†ææ¨¡å¼ï¼ˆquality/speedï¼‰
        user_id: ç”¨æˆ·IDï¼ˆç”¨äºæ—¥å¿—ï¼‰

    Returns:
        DictåŒ…å«åˆ†æç»“æœ
    """
    start_time = time.time()

    try:
        print(f"å¼€å§‹ä½¿ç”¨RPDæ•°æ®åˆ†æå›¾ç‰‡: {image_filename}")
        print(f"RPDæ•°æ®: {rpd_version_data}")

        # 1. è·å–å›¾ç‰‡å°ºå¯¸
        image_width, image_height = await _get_image_dimensions(image_bytes)

        # 2. åˆ›å»ºä¸´æ—¶çš„RPD versionå¯¹è±¡
        temp_rpd_version = await _create_temp_rpd_version_from_data(rpd_version_data)

        # 3. ç”Ÿæˆfindingsï¼ˆä½¿ç”¨ç°æœ‰çš„å•ä¸ªRPDå¤„ç†é€»è¾‘ï¼‰
        model_name = QUALITY_MODEL if mode == AiReviewMode.QUALITY else SPEED_MODEL
        semaphore = asyncio.Semaphore(1)  # å•ä¸ªRPDï¼Œä¸éœ€è¦å¹¶å‘æ§åˆ¶

        findings_data = await _generate_single_rpd_findings(
            image_bytes=image_bytes,
            image_width=image_width,
            image_height=image_height,
            rpd_version=temp_rpd_version,
            semaphore=semaphore,
            model_name=model_name
        )

        # 4. æ ¼å¼åŒ–ç»“æœ
        formatted_findings = []
        for finding_data in findings_data:
            finding_dict = finding_data['finding_dict']

            formatted_finding = {
                "rpd_key": temp_rpd_version.parent_key,
                "rpd_title": temp_rpd_version.title,
                "description": finding_dict['description'],
                "severity": finding_dict['severity'],
                "suggestion": finding_dict.get('suggestion', ''),
                "tag": finding_dict.get('tag', ''),
                "confidence": finding_dict.get('confidence', 0.8),  # é»˜è®¤ç½®ä¿¡åº¦
            }

            # æ·»åŠ è¾¹ç•Œæ¡†ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'bounding_boxes' in finding_data:
                formatted_finding['area'] = finding_data['bounding_boxes']

            formatted_findings.append(formatted_finding)

        processing_time = time.time() - start_time

        result = {
            "success": True,
            "findings": formatted_findings,
            "processing_time": processing_time,
            "image_info": {
                "filename": image_filename,
                "width": image_width,
                "height": image_height
            },
            "rpd_info": {
                "title": temp_rpd_version.title,
                "parent_key": temp_rpd_version.parent_key
            },
            "message": f"RPDæµ‹è¯•å®Œæˆï¼Œå‘ç° {len(formatted_findings)} ä¸ªé—®é¢˜ç‚¹"
        }

        print(f"RPDæµ‹è¯•å®Œæˆ: {image_filename}, å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        return result

    except Exception as e:
        processing_time = time.time() - start_time
        error_message = f"RPDæµ‹è¯•å¤±è´¥: {str(e)}"
        print(error_message)

        traceback.print_exc()

        return {
            "success": False,
            "findings": [],
            "processing_time": processing_time,
            "image_info": {
                "filename": image_filename,
                "width": 0,
                "height": 0
            },
            "error": error_message
        }


async def _create_temp_rpd_version_from_data(rpd_data: Dict[str, Any]) -> ReviewPointDefinitionVersionInDB:
    """
    ä»ä¼ å…¥çš„æ•°æ®åˆ›å»ºä¸´æ—¶çš„RPD versionå¯¹è±¡

    Args:
        rpd_data: åŒ…å«RPD versionæ•°æ®çš„å­—å…¸

    Returns:
        ReviewPointDefinitionVersionInDB: ä¸´æ—¶çš„RPD versionå¯¹è±¡
    """

    # éªŒè¯å¿…éœ€å­—æ®µ
    if not rpd_data.get('title'):
        raise ValueError("RPD title is required")
    if not rpd_data.get('parent_key'):
        raise ValueError("RPD parent_key is required")

    user_instruction = rpd_data.get('user_instruction', '')
    description_for_ai = user_instruction
    if rpd_data['parent_key'] == 'general_ng_review' and rpd_data.get('ng_subcategory'):
        if rpd_data.get('ng_subcategory') == 'abstract_type':
            prompt1, prompt2 = split_review_prompt(user_instruction)
            description_for_ai = prompt1+"\n********\n"+prompt2

    # åˆ›å»ºä¸´æ—¶çš„RPD versionå¯¹è±¡
    temp_rpd_version = ReviewPointDefinitionVersionInDB(
        id=uuid.uuid4(),  # ä¸´æ—¶ID
        review_point_definition_id=uuid.uuid4(),  # ä¸´æ—¶ID
        title=rpd_data['title'],
        parent_key=rpd_data['parent_key'],
        user_instruction=user_instruction,
        description_for_ai=description_for_ai,
        ng_subcategory=rpd_data.get('ng_subcategory', None),
        tag_list=rpd_data.get('tag_list', []),
        reference_images=rpd_data.get('reference_images', []),
        reference_files=rpd_data.get('reference_files', []),
        special_rules=rpd_data.get('special_rules'),
        is_active_version=True,
        version_number=1,
        created_at=datetime.now(timezone.utc),
        created_by="test_user"
    )

    print(
        f"åˆ›å»ºä¸´æ—¶RPD versionå¯¹è±¡: {temp_rpd_version.title} ({temp_rpd_version.parent_key})")
    return temp_rpd_version


# ================================
# å…¶ä»–å†…å®¹ç±»å‹å¤„ç†å‡½æ•°ï¼ˆå­˜æ ¹ï¼‰
# ================================


async def _download_and_process_text(subtask_id: UUID, s3_path: str) -> Tuple[List[Dict[str, Any]], str]:
    """
    è¯´æ˜: ä» S3 ä¸‹è½½å¹¶è§£æ Excelï¼ˆ.xlsxï¼‰å¯¹è¯ç¨¿ï¼Œç”Ÿæˆå¯å¹¶è¡Œå¤„ç†çš„æ–‡æœ¬åˆ†æ®µ `text_parts`ã€‚

    Input
    - subtask_id (UUID): å­ä»»åŠ¡ IDï¼Œç”¨äºæ—¥å¿—æˆ–é“¾è·¯è¿½è¸ªã€‚
    - s3_path (str): S3 æ–‡ä»¶è·¯å¾„ï¼Œä»…æ”¯æŒä»¥ .xlsx ç»“å°¾çš„ Excel æ–‡ä»¶ã€‚

    Output
    - Tuple[List[Dict[str, Any]], str]: åŒ…å«ï¼š
      - text_parts: æ–‡æœ¬åˆ†æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
        - original_index (int): æº Excel ä¸­çš„è¡Œç´¢å¼•ï¼ˆä¸ DataFrame è¡Œå·ä¸€è‡´ï¼‰
        - speaker (str): ã€Œè©±è€…ã€åˆ—çš„å€¼
        - line (str): æ¸…æ´—åçš„æ­£æ–‡ï¼ˆå»é™¤ â˜…ã€â€»ã€#ã€æ¢è¡Œã€å…¨è§’ç©ºæ ¼ï¼‰
        - sheet_index (int): å·¥ä½œè¡¨åºå·ï¼Œä» 0 å¼€å§‹
      - local_file_path: æœ¬åœ°ä¸´æ—¶Excelæ–‡ä»¶è·¯å¾„

    çº¦æŸ/è¡Œä¸º: 
      - æ¯ä¸ªå·¥ä½œè¡¨ `skiprows=11`, `engine='openpyxl'`
      - æ­£æ–‡åˆ—åå›ºå®šä¸º "æœ¬æ–‡ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–"
      - æ— æ³•è¯»å–çš„å·¥ä½œè¡¨å°†è¢«è·³è¿‡
    - è‹¥ `s3_path` é `.xlsx` å°†æŠ›å‡º `ValueError`
    """
    if not s3_path or not s3_path.lower().endswith(".xlsx"):
        raise ValueError("åªæ”¯æŒå¤„ç† .xlsx æ–‡ä»¶")

    # ä¸‹è½½ S3 æ–‡ä»¶å†…å®¹åˆ°ä¸´æ—¶æ–‡ä»¶
    excel_bytes = download_file_content_from_s3_sync(s3_path)

    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp_file:
        tmp_file.write(excel_bytes)
        local_file_path = tmp_file.name

    # ä»æœ¬åœ°æ–‡ä»¶åˆ›å»ºbufferç”¨äºpandasè¯»å–
    bytes_buffer = io.BytesIO(excel_bytes)

    # ä¸ batch.speaker_check_process_s3_file å¯¹é½çš„åˆ—å
    text_col = "æœ¬æ–‡ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–"

    try:
        xls = pd.ExcelFile(bytes_buffer)
    except Exception as exc:
        raise ValueError(f"è¯»å–Excelå¤±è´¥: {exc}")

    text_parts: List[Dict[str, Any]] = []

    for sheet_index, sheet_name in enumerate(xls.sheet_names):
        try:
            df = pd.read_excel(
                xls,
                sheet_name=sheet_name,
                skiprows=11,
                engine="openpyxl",
            )
        except Exception:
            # è·³è¿‡æ— æ³•è¯»å–çš„å·¥ä½œè¡¨
            continue

        for row_index, row in df.iterrows():
            speaker = row.get("è©±è€…")
            line = row.get(text_col)
            if pd.notna(speaker) and pd.notna(line):
                clean_line = (
                    str(line)
                    .replace("â˜…", "")
                    .replace("â€»", "")
                    .replace("#", "")
                    .replace("\n", "")
                    .replace("\u3000", "")
                )
                text_parts.append(
                    {
                        "original_index": row_index,
                        "speaker": speaker,
                        "line": clean_line,
                        "sheet_index": sheet_index,
                    }
                )
    return text_parts, local_file_path


async def _download_and_process_audio(subtask_id: UUID, s3_path: str) -> tuple[bytes, dict]:
    """ä¸‹è½½å¹¶å¤„ç†éŸ³é¢‘ï¼Œè¿”å›éŸ³é¢‘å­—èŠ‚æ•°æ®å’Œå…ƒæ•°æ®

    Args:
        subtask_id: å­ä»»åŠ¡ID
        s3_path: S3è·¯å¾„

    Returns:
        tuple: (éŸ³é¢‘å­—èŠ‚æ•°æ®, éŸ³é¢‘å…ƒæ•°æ®å­—å…¸)

    TODO: å®ç°éŸ³é¢‘å¤„ç†é€»è¾‘
    - ä¸‹è½½éŸ³é¢‘æ–‡ä»¶
    - æå–éŸ³é¢‘ç‰¹å¾
    - è·å–éŸ³é¢‘å…ƒæ•°æ®ï¼ˆæ—¶é•¿ã€æ ¼å¼ç­‰ï¼‰
    - å¯èƒ½éœ€è¦è¯­éŸ³è½¬æ–‡æœ¬
    """
    raise NotImplementedError("éŸ³é¢‘å¤„ç†åŠŸèƒ½å°šæœªå®ç°")


# async def _split_audio_into_segments(audio_data: bytes, audio_metadata: dict, segment_duration_seconds: int = 30) -> List[Dict[str, Any]]:
#     """
#     å°†é•¿éŸ³é¢‘åˆ†å‰²æˆå¤šä¸ªç‰‡æ®µä»¥ä¾¿å¹¶è¡Œå¤„ç†

#     Args:
#         audio_data: å®Œæ•´çš„éŸ³é¢‘å­—èŠ‚æ•°æ®
#         audio_metadata: éŸ³é¢‘å…ƒæ•°æ®ï¼ˆåŒ…å«æ—¶é•¿ã€é‡‡æ ·ç‡ç­‰ä¿¡æ¯ï¼‰
#         segment_duration_seconds: æ¯ä¸ªç‰‡æ®µçš„æ—¶é•¿ï¼ˆç§’ï¼‰

#     Returns:
#         List[Dict[str, Any]]: éŸ³é¢‘ç‰‡æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {'data': bytes, 'metadata': dict}

#     TODO: å®ç°æ™ºèƒ½éŸ³é¢‘åˆ†å‰²é€»è¾‘
#     - æŒ‰å›ºå®šæ—¶é•¿åˆ†å‰²
#     - æŒ‰é™éŸ³æ£€æµ‹åˆ†å‰²
#     - æŒ‰è¯­éŸ³æ®µè½åˆ†å‰²
#     - ä¿æŒéŸ³é¢‘è´¨é‡
#     """
#     if not audio_data:
#         return []

#     audio_segments = []
#     total_duration = audio_metadata.get('duration_seconds', 0)

#     if total_duration == 0:
#         print("éŸ³é¢‘æ—¶é•¿ä¿¡æ¯ç¼ºå¤±ï¼Œæ— æ³•åˆ†å‰²")
#         return []

#     # ç®€å•çš„åŸºäºæ—¶é•¿çš„åˆ†å‰²å®ç°ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨éŸ³é¢‘å¤„ç†åº“å¦‚librosaã€pydubç­‰ï¼‰
#     # å®é™…å®ç°ä¸­åº”è¯¥è€ƒè™‘é‡‡æ ·ç‡ã€å£°é“æ•°ç­‰éŸ³é¢‘å‚æ•°
#     bytes_per_second = len(audio_data) / \
#         total_duration if total_duration > 0 else 0

#     for i in range(0, int(total_duration), segment_duration_seconds):
#         start_time = i
#         end_time = min(i + segment_duration_seconds, total_duration)

#         # è®¡ç®—å­—èŠ‚èŒƒå›´ï¼ˆè¿™æ˜¯ç®€åŒ–çš„è®¡ç®—ï¼Œå®é™…éœ€è¦è€ƒè™‘éŸ³é¢‘æ ¼å¼ï¼‰
#         start_byte = int(start_time * bytes_per_second)
#         end_byte = int(end_time * bytes_per_second)

#         segment_data = audio_data[start_byte:end_byte]
#         segment_metadata = {
#             'segment_number': len(audio_segments) + 1,
#             'start_time': start_time,
#             'end_time': end_time,
#             'duration': end_time - start_time,
#             'total_segments': (int(total_duration) + segment_duration_seconds - 1) // segment_duration_seconds,
#             'original_metadata': audio_metadata
#         }

#         audio_segments.append({
#             'data': segment_data,
#             'metadata': segment_metadata
#         })

#     print(f"éŸ³é¢‘å·²åˆ†å‰²ä¸º {len(audio_segments)} ä¸ªç‰‡æ®µ")
#     return audio_segments


# async def _generate_findings_for_video(
#     video_data: bytes,
#     video_metadata: dict,
#     active_rpd_versions: List,
#     ai_review_orm,
#     mode: AiReviewMode
# ) -> List:
#     """ä¸ºè§†é¢‘å†…å®¹ç”ŸæˆAIå®¡æŸ¥ç»“æœ

#     Args:
#         video_data: è§†é¢‘å­—èŠ‚æ•°æ®
#         video_metadata: è§†é¢‘å…ƒæ•°æ®
#         active_rpd_versions: æ´»è·ƒçš„å®¡æŸ¥ç‚¹å®šä¹‰ç‰ˆæœ¬åˆ—è¡¨
#         ai_review_orm: AIå®¡æŸ¥ORMå¯¹è±¡
#         mode: å®¡æŸ¥æ¨¡å¼

#     Returns:
#         List: AIå®¡æŸ¥å‘ç°æ¡ç›®åˆ—è¡¨

#     TODO: å®ç°è§†é¢‘AIå®¡æŸ¥é€»è¾‘
#     - å¯¹å…³é”®å¸§è¿›è¡Œå›¾åƒå®¡æŸ¥
#     - æ£€æµ‹è§†é¢‘ä¸­çš„è¿è§„å†…å®¹
#     - åˆ†æè§†é¢‘åœºæ™¯å’Œå†…å®¹
#     """
#     raise NotImplementedError("è§†é¢‘AIå®¡æŸ¥åŠŸèƒ½å°šæœªå®ç°")


async def _generate_findings_for_text(
    text_content: str,
    text_metadata: dict,
    active_rpd_versions: List,
    ai_review_orm,
    mode: AiReviewMode
) -> List:
    """ä¸ºæ–‡æœ¬å†…å®¹ç”ŸæˆAIå®¡æŸ¥ç»“æœ

    Args:
        text_content: æ–‡æœ¬å†…å®¹
        text_metadata: æ–‡æœ¬å…ƒæ•°æ®
        active_rpd_versions: æ´»è·ƒçš„å®¡æŸ¥ç‚¹å®šä¹‰ç‰ˆæœ¬åˆ—è¡¨
        ai_review_orm: AIå®¡æŸ¥ORMå¯¹è±¡
        mode: å®¡æŸ¥æ¨¡å¼

    Returns:
        List: AIå®¡æŸ¥å‘ç°æ¡ç›®åˆ—è¡¨

    TODO: å®ç°æ–‡æœ¬AIå®¡æŸ¥é€»è¾‘
    - æ£€æµ‹æ–‡æœ¬ä¸­çš„è¿è§„å†…å®¹
    - åˆ†ææ–‡æœ¬æƒ…æ„Ÿå’Œå†…å®¹
    - æ£€æŸ¥æ–‡æœ¬æ ¼å¼å’Œè´¨é‡
    """
    raise NotImplementedError("æ–‡æœ¬AIå®¡æŸ¥åŠŸèƒ½å°šæœªå®ç°")


# async def _generate_findings_for_audio(
#     audio_data: bytes,
#     audio_metadata: dict,
#     active_rpd_versions: List,
#     ai_review_orm,
#     mode: AiReviewMode
# ) -> List:
#     """ä¸ºéŸ³é¢‘å†…å®¹ç”ŸæˆAIå®¡æŸ¥ç»“æœ

#     Args:
#         audio_data: éŸ³é¢‘å­—èŠ‚æ•°æ®
#         audio_metadata: éŸ³é¢‘å…ƒæ•°æ®
#         active_rpd_versions: æ´»è·ƒçš„å®¡æŸ¥ç‚¹å®šä¹‰ç‰ˆæœ¬åˆ—è¡¨
#         ai_review_orm: AIå®¡æŸ¥ORMå¯¹è±¡
#         mode: å®¡æŸ¥æ¨¡å¼

#     Returns:
#         List: AIå®¡æŸ¥å‘ç°æ¡ç›®åˆ—è¡¨

#     TODO: å®ç°éŸ³é¢‘AIå®¡æŸ¥é€»è¾‘
#     - è¯­éŸ³è½¬æ–‡æœ¬åè¿›è¡Œæ–‡æœ¬å®¡æŸ¥
#     - æ£€æµ‹éŸ³é¢‘ä¸­çš„è¿è§„å†…å®¹
#     - åˆ†æéŸ³é¢‘è´¨é‡å’Œå†…å®¹
#     """
#     raise NotImplementedError("éŸ³é¢‘AIå®¡æŸ¥åŠŸèƒ½å°šæœªå®ç°")


# async def _process_single_audio_segment(
#     audio_segment: bytes,
#     audio_metadata: Dict[str, Any],
#     content_type: SubtaskType,
#     active_rpd_versions: List[ReviewPointDefinitionVersionInDB],
#     ai_review_orm: AiReview,
#     mode: AiReviewMode,
#     semaphore: asyncio.Semaphore
# ) -> List[AiReviewFindingEntry]:
#     """
#     å¤„ç†å•ä¸ªéŸ³é¢‘ç‰‡æ®µï¼Œä¸ºå…¶ç”Ÿæˆæ‰€æœ‰RPDçš„findings
#     """
#     async with semaphore:
#         try:
#             print(
#                 f"å¤„ç†éŸ³é¢‘ç‰‡æ®µ - {audio_metadata.get('segment_number', 'unknown')}")

#             # è¿™é‡Œéœ€è¦å®ç°éŸ³é¢‘çš„AIå®¡æŸ¥é€»è¾‘
#             # ç›®å‰å…ˆè¿”å›ç©ºåˆ—è¡¨ï¼Œç­‰å¾…éŸ³é¢‘å¤„ç†åŠŸèƒ½çš„å®Œæ•´å®ç°
#             findings = await _generate_findings_for_audio(
#                 audio_segment, audio_metadata, active_rpd_versions, ai_review_orm, mode
#             )

#             print(
#                 f"éŸ³é¢‘ç‰‡æ®µ {audio_metadata.get('segment_number', 'unknown')} å¤„ç†å®Œæˆ")
#             return findings

#         except NotImplementedError:
#             print(
#                 f"éŸ³é¢‘å¤„ç†åŠŸèƒ½å°šæœªå®ç°ï¼Œè·³è¿‡éŸ³é¢‘ç‰‡æ®µ {audio_metadata.get('segment_number', 'unknown')}")
#             return []
#         except Exception as e:
#             print(f"å¤„ç†éŸ³é¢‘ç‰‡æ®µæ—¶å‡ºé”™: {e}")
#             return []


# async def _generate_findings_for_audio_segments_parallel(
#     # æ¯ä¸ªå…ƒç´ åŒ…å« {'data': bytes, 'metadata': dict}
#     audio_segments: List[Dict[str, Any]],
#     content_type: SubtaskType,
#     active_rpd_versions: List[ReviewPointDefinitionVersionInDB],
#     ai_review_orm: AiReview,
#     mode: AiReviewMode,
# ) -> List[AiReviewFindingEntry]:
#     """
#     å¹¶è¡Œå¤„ç†æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µï¼Œä¸ºæ¯ä¸ªç‰‡æ®µç”ŸæˆAIå®¡æŸ¥findings
#     """
#     start_time = time.time()

#     print(f"å¼€å§‹å¹¶è¡Œå¤„ç† {len(audio_segments)} ä¸ªéŸ³é¢‘ç‰‡æ®µ")

#     # è®¾ç½®å¹¶å‘ä¿¡å·é‡
#     semaphore = asyncio.Semaphore(max_concurrent)

#     # åˆ›å»ºæ‰€æœ‰éŸ³é¢‘ç‰‡æ®µå¤„ç†ä»»åŠ¡
#     tasks = []
#     for i, audio_segment_data in enumerate(audio_segments):
#         audio_data = audio_segment_data['data']
#         audio_metadata = audio_segment_data['metadata']

#         task = asyncio.create_task(
#             _process_single_audio_segment(
#                 audio_data, audio_metadata, content_type,
#                 active_rpd_versions, ai_review_orm, mode, semaphore
#             ),
#             name=f"audio_segment_task_{i}"
#         )
#         tasks.append(task)

#     print(f"æ‰€æœ‰ {len(tasks)} ä¸ªéŸ³é¢‘ç‰‡æ®µä»»åŠ¡å·²åˆ›å»ºï¼Œå¼€å§‹ç­‰å¾…å®Œæˆ...")

#     # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
#     done, pending = await asyncio.wait(tasks, return_when=asyncio.ALL_COMPLETED)

#     # æ”¶é›†æ‰€æœ‰ç»“æœ
#     all_findings = []
#     for task in done:
#         try:
#             findings = await task
#             all_findings.extend(findings)
#         except Exception as e:
#             print(f"è·å–éŸ³é¢‘ç‰‡æ®µå¤„ç†ç»“æœæ—¶å‡ºé”™: {e}")

#     end_time = time.time()
#     print(f"éŸ³é¢‘ç‰‡æ®µå¹¶è¡Œå¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
#     print(f"å…±ç”Ÿæˆ {len(all_findings)} ä¸ªfindings")

#     return all_findings

    # except Exception as db_error:
    #     logger.error(f"ğŸ—„ï¸ [æ‰¹æ¬¡ {batch_id}] æ›´æ–°æ•°æ®åº“è®°å½•å¤±è´¥: {str(db_error)}")
    #     # å³ä½¿æ•°æ®åº“æ›´æ–°å¤±è´¥ï¼Œä¹Ÿä¸å½±å“ä¸»è¦ä»»åŠ¡çš„å®Œæˆ


async def interrupt_ai_review_for_subtask(subtask_id: UUID) -> bool:
    """
    ä¸­æ–­æŒ‡å®šå­ä»»åŠ¡çš„AIå®¡æŸ¥å¤„ç†ã€‚

    Args:
        subtask_id: å­ä»»åŠ¡ID

    Returns:
        bool: å¦‚æœæˆåŠŸä¸­æ–­è¿”å›Trueï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯ä¸­æ–­çš„reviewè¿”å›False
    """
    try:
        # è·å–æœ€æ–°çš„AI review
        latest_review = await get_latest_ai_review_for_subtask(subtask_id)

        if not latest_review:
            logger.info(f"No AI review found for subtask {subtask_id}")
            return False

        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä¸­æ–­ï¼ˆåªæœ‰processingçŠ¶æ€çš„å¯ä»¥ä¸­æ–­ï¼‰
        if latest_review.processing_status != AiReviewProcessingStatus.PROCESSING.value:
            logger.info(
                f"AI review for subtask {subtask_id} is not in processing state, current state: {latest_review.processing_status}")
            return False

        # è·å–AiReview ORMå¯¹è±¡å¹¶æ›´æ–°çŠ¶æ€ä¸ºcancelled
        ai_review_orm = await AiReview.get(id=latest_review.id).prefetch_related('subtask')

        # é¦–å…ˆè®¾ç½®ä¸­æ–­ä¿¡å·ï¼Œè®©æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡èƒ½å¤Ÿæ£€æµ‹åˆ°
        logger.info(
            f"Setting cancellation signal for AI review {latest_review.id}")
        ai_review_orm.should_cancel = True
        await ai_review_orm.save(update_fields=['should_cancel'])

        # ç„¶åæ›´æ–°å¤„ç†çŠ¶æ€
        logger.info(
            f"Updating processing status to CANCELLED for AI review {latest_review.id}")
        await _update_processing_status(
            ai_review_orm,
            AiReviewProcessingStatus.CANCELLED,
            error_message="AI review was interrupted by user"
        )

        # æ‰§è¡Œå›é€€é€»è¾‘
        logger.info(
            f"AI review cancelled for subtask {subtask_id}, attempting rollback...")
        await _rollback_failed_review(ai_review_orm)

        logger.info(
            f"Successfully interrupted AI review {latest_review.id} for subtask {subtask_id}")
        return True

    except Exception as e:
        logger.error(
            f"Error interrupting AI review for subtask {subtask_id}: {e}")
        traceback.print_exc()
        return False

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDFå›¾ç‰‡æå–Pipeline
åŠŸèƒ½ï¼šä»PDFä¸­æå–æ‰€æœ‰å›¾ç‰‡ï¼Œå¹¶å°è¯•ç»™å‡ºæœ‰æ„ä¹‰çš„æ–‡ä»¶å
"""

import argparse
import hashlib
import io
import json
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Any, BinaryIO, Dict, List, Optional, Tuple

import fitz  # type: ignore
from PIL import Image, ImageOps


class PDFImageExtractor:
    """PDFå›¾ç‰‡æå–å™¨"""

    def __init__(self, output_dir: str = "extracted_images"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.extraction_log: List[Dict[str, Any]] = []
        self.duplicate_hashes: set[str] = set()

    def clean_filename(self, text: str, max_length: int = 50) -> str:
        """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦ï¼Œé’ˆå¯¹è§’è‰²å‚è€ƒå›¾ä¼˜åŒ–"""
        if not text or not text.strip():
            return ""

        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text.strip())

        # ä¿ç•™æ›´å¤šæœ‰ç”¨å­—ç¬¦ï¼šä¸­è‹±æ–‡ã€æ•°å­—ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦ã€ç‚¹å·ï¼ˆç”¨äºæ‰©å±•åï¼‰
        cleaned = re.sub(r'[^\w\u4e00-\u9fff\-\.\s]', '_', text)

        # ç‰¹æ®Šå¤„ç†ï¼šä¿ç•™å¸¸è§çš„æ–‡ä»¶æ‰©å±•ååˆ†éš”ç¬¦
        cleaned = re.sub(r'_+', '_', cleaned)  # åˆå¹¶å¤šä¸ªä¸‹åˆ’çº¿
        cleaned = cleaned.strip('_. ')  # ç§»é™¤é¦–å°¾çš„ä¸‹åˆ’çº¿ã€ç‚¹å·å’Œç©ºæ ¼

        # å¦‚æœåŒ…å«æ–‡ä»¶æ‰©å±•åï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
        if '.' in cleaned:
            parts = cleaned.split('.')
            if len(parts) >= 2:
                name_part = parts[0]
                ext_part = parts[-1].lower()
                # å¦‚æœæ˜¯å¸¸è§æ‰©å±•åï¼Œä¿ç•™ï¼›å¦åˆ™æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
                if ext_part in ['psd', 'ai', 'png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff', 'webp']:
                    cleaned = f"{name_part}.{ext_part}"
                else:
                    cleaned = '_'.join(parts)

        # é™åˆ¶é•¿åº¦
        if len(cleaned) > max_length:
            if '.' in cleaned:
                # å¦‚æœæœ‰æ‰©å±•åï¼Œä¿ç•™æ‰©å±•å
                name, ext = cleaned.rsplit('.', 1)
                max_name_length = max_length - len(ext) - 1
                cleaned = f"{name[:max_name_length].rstrip('_')}.{ext}"
            else:
                cleaned = cleaned[:max_length].rstrip('_')

        return cleaned if cleaned else ""

    def get_image_hash(self, image_data: bytes) -> str:
        """è·å–å›¾ç‰‡çš„MD5å“ˆå¸Œå€¼ç”¨äºå»é‡"""
        return hashlib.md5(image_data).hexdigest()

    def find_nearby_text(self, page, img_rect, search_radius: int = 100) -> List[str]:
        """æŸ¥æ‰¾å›¾ç‰‡å‘¨å›´çš„æ–‡æœ¬"""
        try:
            # æ‰©å±•æœç´¢åŒºåŸŸ
            search_rect = fitz.Rect(
                img_rect.x0 - search_radius,
                img_rect.y0 - search_radius,
                img_rect.x1 + search_radius,
                img_rect.y1 + search_radius
            )

            # è·å–æ–‡æœ¬å—
            text_blocks = page.get_text("dict")
            nearby_texts = []

            for block in text_blocks.get("blocks", []):
                if "lines" not in block:
                    continue

                for line in block["lines"]:
                    line_rect = fitz.Rect(line["bbox"])

                    # æ£€æŸ¥æ–‡æœ¬å—æ˜¯å¦åœ¨æœç´¢åŒºåŸŸå†…
                    if search_rect.intersects(line_rect):
                        line_text = ""
                        for span in line["spans"]:
                            line_text += span.get("text", "")

                        line_text = line_text.strip()
                        if line_text and len(line_text) > 2:
                            # è®¡ç®—ä¸å›¾ç‰‡çš„è·ç¦»
                            distance = min(
                                abs(line_rect.y1 - img_rect.y0),  # ä¸Šæ–¹è·ç¦»
                                abs(line_rect.y0 - img_rect.y1),  # ä¸‹æ–¹è·ç¦»
                                abs(line_rect.x1 - img_rect.x0),  # å·¦ä¾§è·ç¦»
                                abs(line_rect.x0 - img_rect.x1)   # å³ä¾§è·ç¦»
                            )
                            nearby_texts.append((distance, line_text))

            # æŒ‰è·ç¦»æ’åºï¼Œè¿”å›æœ€è¿‘çš„æ–‡æœ¬
            nearby_texts.sort(key=lambda x: x[0])
            return [text for _, text in nearby_texts[:3]]

        except Exception as e:
            print(f"è·å–å‘¨å›´æ–‡æœ¬æ—¶å‡ºé”™: {e}")
            return []

    def get_image_name_candidates(self, doc, xref: int, page_num: int,
                                  img_index: int, img_rect=None, page=None) -> List[str]:
        """è·å–å›¾ç‰‡åç§°å€™é€‰åˆ—è¡¨"""
        candidates = []

        # 1. å°è¯•è·å–PDFå†…éƒ¨åç§°
        try:
            img_name = doc.xref_get_key(xref, "Name")
            if img_name and img_name[1]:
                internal_name = img_name[1].strip('/')
                if internal_name:
                    candidates.append(f"internal_{internal_name}")
        except:
            pass

        # 2. å°è¯•è·å–å¯¹è±¡å­—å…¸ä¸­çš„å…¶ä»–ä¿¡æ¯
        try:
            obj_dict = doc.xref_get_keys(xref)
            for key in ['Title', 'Subject', 'Alt', 'ActualText']:
                if key in obj_dict:
                    value = doc.xref_get_key(xref, key)
                    if value and value[1]:
                        candidates.append(f"attr_{value[1]}")
        except:
            pass

        # 3. åŸºäºå‘¨å›´æ–‡æœ¬ç”Ÿæˆåç§°
        if img_rect and page:
            nearby_texts = self.find_nearby_text(page, img_rect)
            if nearby_texts:
                # ç»„åˆæœ€ç›¸å…³çš„æ–‡æœ¬
                combined_text = "_".join(nearby_texts[:2])
                if combined_text:
                    candidates.append(f"context_{combined_text}")

        # 4. åŸºäºé¡µé¢å†…å®¹å¯»æ‰¾æ–‡ä»¶åæ¨¡å¼
        if page:
            try:
                # è·å–é¡µé¢ä¸­çš„æ‰€æœ‰æ–‡æœ¬ï¼Œå¯»æ‰¾å¯èƒ½çš„æ–‡ä»¶å
                page_text = page.get_text()

                # å¯»æ‰¾å¸¸è§çš„æ–‡ä»¶åæ¨¡å¼ï¼ˆåŒ…æ‹¬è®¾è®¡æ–‡ä»¶å’Œå›¾ç‰‡æ–‡ä»¶ï¼‰
                filename_patterns = [
                    # æ–‡ä»¶å.æ‰©å±•å
                    r'([a-zA-Z0-9_\-\u4e00-\u9fff]+\.(?:psd|ai|png|jpg|jpeg|gif|bmp|tiff|webp))',
                    # æå–æ–‡ä»¶åéƒ¨åˆ†
                    r'([a-zA-Z0-9_\-\u4e00-\u9fff]{3,20})\.(?:psd|ai|png|jpg|jpeg)',
                    # æ ‡ç­¾åçš„åç§°
                    r'(?:æ–‡ä»¶å|filename|name)[:ï¼š\s]*([a-zA-Z0-9_\-\u4e00-\u9fff]{3,30})',
                    # å‚è€ƒå›¾å‘½å
                    r'([a-zA-Z0-9_\-\u4e00-\u9fff]{3,20})(?:\s*å‚è€ƒ|_ref|_reference)',
                    # è§’è‰²åç§°
                    r'(?:è§’è‰²|character|äººç‰©)[:ï¼š\s]*([a-zA-Z0-9_\-\u4e00-\u9fff]{2,20})',
                ]

                for pattern in filename_patterns:
                    matches = re.finditer(pattern, page_text, re.IGNORECASE)
                    for match in matches:
                        filename_candidate = match.group(1).strip()
                        if filename_candidate and len(filename_candidate) >= 2:
                            candidates.append(f"file_{filename_candidate}")

            except:
                pass

        return candidates

    def generate_final_filename(self, candidates: List[str], img_ext: str,
                                page_num: int, img_index: int, img_hash: str) -> str:
        """ä»å€™é€‰åç§°ä¸­é€‰æ‹©æœ€ä½³æ–‡ä»¶å"""

        # æ¸…ç†å’Œè¯„åˆ†å€™é€‰åç§°
        scored_candidates = []

        for candidate in candidates:
            cleaned = self.clean_filename(candidate)
            if cleaned and len(cleaned) >= 3:  # æœ€çŸ­3ä¸ªå­—ç¬¦
                score = len(cleaned)  # åŸºç¡€åˆ†æ•°ï¼šé•¿åº¦

                # åŠ åˆ†é¡¹ - é’ˆå¯¹è§’è‰²å‚è€ƒå›¾ä¼˜åŒ–
                if 'file_' in candidate:  # æ–‡ä»¶åæ¨¡å¼åŒ¹é…
                    score += 25
                if 'internal_' in candidate:  # PDFå†…éƒ¨åç§°
                    score += 20
                if any(ext in cleaned.lower() for ext in ['psd', 'ai', 'png', 'jpg', 'jpeg']):
                    score += 15  # åŒ…å«æ–‡ä»¶æ‰©å±•å
                if any(keyword in cleaned.lower() for keyword in ['è§’è‰²', 'character', 'äººç‰©', 'ref', 'reference']):
                    score += 12  # åŒ…å«è§’è‰²ç›¸å…³å…³é”®è¯
                if any(char.isdigit() for char in cleaned):  # åŒ…å«æ•°å­—ï¼ˆç‰ˆæœ¬å·ç­‰ï¼‰
                    score += 8

                # å‡åˆ†é¡¹
                if 'context_' in candidate and len(cleaned) < 10:
                    score -= 5
                if any(word in cleaned.lower() for word in ['figure', 'table', 'chart', 'å›¾', 'è¡¨', 'å›¾è¡¨']):
                    score -= 10  # é™ä½ä¼ ç»Ÿå›¾è¡¨å‘½åçš„ä¼˜å…ˆçº§

                scored_candidates.append((score, cleaned))

        # é€‰æ‹©æœ€é«˜åˆ†çš„å€™é€‰åç§°
        if scored_candidates:
            scored_candidates.sort(key=lambda x: x[0], reverse=True)
            best_name = scored_candidates[0][1]

            # é¿å…æ–‡ä»¶åå†²çª
            base_name = best_name
            counter = 1
            final_path = self.output_dir / f"{base_name}.{img_ext}"

            while final_path.exists():
                final_path = self.output_dir / \
                    f"{base_name}_{counter}.{img_ext}"
                counter += 1

            return final_path.name

        # å›é€€åˆ°é»˜è®¤å‘½å
        return f"page_{page_num:03d}_img_{img_index:02d}_{img_hash[:8]}.{img_ext}"

    def generate_thumbnail(self, image_data: bytes, max_size: int = 300) -> bytes:
        """ç”Ÿæˆç¼©ç•¥å›¾"""
        try:
            # æ‰“å¼€å›¾ç‰‡
            image = Image.open(io.BytesIO(image_data))

            # è½¬æ¢ä¸ºRGBæ¨¡å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if image.mode not in ('RGB', 'RGBA'):
                image = image.convert('RGB')

            # ç”Ÿæˆç¼©ç•¥å›¾ï¼Œä¿æŒå®½é«˜æ¯”
            image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)

            # ä¿å­˜ä¸ºJPEGæ ¼å¼
            output = io.BytesIO()
            image.save(output, format='JPEG', quality=85, optimize=True)
            return output.getvalue()

        except Exception as e:
            print(f"ç”Ÿæˆç¼©ç•¥å›¾å¤±è´¥: {e}")
            # å¦‚æœç¼©ç•¥å›¾ç”Ÿæˆå¤±è´¥ï¼Œè¿”å›åŸå›¾æ•°æ®ï¼ˆå¯èƒ½ä¼šæ¯”è¾ƒå¤§ï¼‰
            return image_data

    async def extract_images_for_preview(self, pdf_data: BinaryIO,
                                         skip_duplicates: bool = True,
                                         min_size: int = 1000,
                                         thumbnail_size: int = 300) -> Dict[str, Any]:
        """ä»PDFæå–å›¾ç‰‡å¹¶ç”Ÿæˆé¢„è§ˆæ•°æ®ï¼ˆç”¨äºä¸´æ—¶å­˜å‚¨ï¼‰"""

        # é‡ç½®é‡å¤æ£€æµ‹çŠ¶æ€
        self.duplicate_hashes.clear()

        try:
            # è¯»å–PDFæ•°æ®
            pdf_data.seek(0)
            pdf_bytes = pdf_data.read()
            doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        except Exception as e:
            raise Exception(f"æ— æ³•æ‰“å¼€PDFæ–‡ä»¶: {e}")

        extraction_result: Dict[str, Any] = {
            "total_pages": len(doc),
            "total_images_found": 0,
            "images_extracted": 0,
            "duplicates_skipped": 0,
            "small_images_skipped": 0,
            "errors": [],
            "extracted_images": []  # åŒ…å«å›¾ç‰‡æ•°æ®çš„åˆ—è¡¨
        }

        print(f"PDFå…±æœ‰ {len(doc)} é¡µ")

        for page_num in range(len(doc)):
            try:
                page = doc.load_page(page_num)
                image_list = page.get_images(full=True)

                extraction_result["total_images_found"] += len(image_list)

                for img_index, img in enumerate(image_list):
                    try:
                        xref = img[0]

                        # è·å–å›¾ç‰‡æ•°æ®
                        img_dict = doc.extract_image(xref)
                        img_data = img_dict["image"]
                        img_ext = img_dict["ext"]

                        # æ£€æŸ¥å›¾ç‰‡å¤§å°
                        if len(img_data) < min_size:
                            extraction_result["small_images_skipped"] += 1
                            continue

                        # æ£€æŸ¥é‡å¤
                        img_hash = self.get_image_hash(img_data)
                        if skip_duplicates and img_hash in self.duplicate_hashes:
                            extraction_result["duplicates_skipped"] += 1
                            print(
                                f"  â© è·³è¿‡é‡å¤å›¾ç‰‡: {img_hash[:8]}... (ç¬¬{page_num+1}é¡µ)")
                            continue

                        self.duplicate_hashes.add(img_hash)
                        print(f"  âœ“ æ–°å›¾ç‰‡å“ˆå¸Œ: {img_hash[:8]}... (ç¬¬{page_num+1}é¡µ)")

                        # è·å–å›¾ç‰‡ä½ç½®ä¿¡æ¯
                        img_rect = None
                        try:
                            img_rects = page.get_image_rects(xref)
                            if img_rects:
                                img_rect = img_rects[0]
                        except:
                            pass

                        # ç”Ÿæˆæ–‡ä»¶åå€™é€‰
                        candidates = self.get_image_name_candidates(
                            doc, xref, page_num, img_index, img_rect, page
                        )

                        # é€‰æ‹©æœ€ç»ˆæ–‡ä»¶å
                        filename = self.generate_final_filename(
                            candidates, img_ext, page_num, img_index, img_hash
                        )

                        # ç”Ÿæˆç¼©ç•¥å›¾
                        thumbnail_data = self.generate_thumbnail(
                            img_data, thumbnail_size)

                        # è®°å½•æå–ä¿¡æ¯
                        extract_info = {
                            "filename": filename,
                            "original_filename": filename,  # ä¿æŒä¸€è‡´
                            "page": page_num + 1,
                            "index": img_index,
                            "size_bytes": len(img_data),
                            "format": img_ext,
                            "hash": img_hash,
                            "candidates_tried": candidates,
                            "dimensions": f"{img_dict.get('width', 'unknown')}x{img_dict.get('height', 'unknown')}",
                            "original_data": img_data,  # åŸå§‹å›¾ç‰‡æ•°æ®
                            "thumbnail_data": thumbnail_data,  # ç¼©ç•¥å›¾æ•°æ®
                            "thumbnail_size_bytes": len(thumbnail_data)
                        }

                        if img_rect:
                            extract_info["position"] = {
                                "x": round(img_rect.x0, 2),
                                "y": round(img_rect.y0, 2),
                                "width": round(img_rect.width, 2),
                                "height": round(img_rect.height, 2)
                            }

                        extraction_result["extracted_images"].append(
                            extract_info)
                        extraction_result["images_extracted"] += 1

                        print(
                            f"  âœ“ æå–: {filename} (åŸå›¾: {len(img_data)} bytes, ç¼©ç•¥å›¾: {len(thumbnail_data)} bytes)")

                    except Exception as e:
                        error_msg = f"ç¬¬{page_num+1}é¡µå›¾ç‰‡{img_index}æå–å¤±è´¥: {e}"
                        extraction_result["errors"].append(error_msg)
                        print(f"  âœ— {error_msg}")

            except Exception as e:
                error_msg = f"å¤„ç†ç¬¬{page_num+1}é¡µæ—¶å‡ºé”™: {e}"
                extraction_result["errors"].append(error_msg)
                print(f"âœ— {error_msg}")

        doc.close()
        return extraction_result

    def extract_images_from_pdf(self, pdf_path: str,
                                skip_duplicates: bool = True,
                                min_size: int = 1000) -> Dict[str, Any]:
        """ä»PDFæå–æ‰€æœ‰å›¾ç‰‡"""

        # é‡ç½®é‡å¤æ£€æµ‹çŠ¶æ€
        self.duplicate_hashes.clear()

        print(f"å¼€å§‹å¤„ç†PDFæ–‡ä»¶: {pdf_path}")

        try:
            doc = fitz.open(pdf_path)
        except Exception as e:
            raise Exception(f"æ— æ³•æ‰“å¼€PDFæ–‡ä»¶: {e}")

        extraction_stats: Dict[str, Any] = {
            "total_pages": len(doc),
            "total_images_found": 0,
            "images_extracted": 0,
            "duplicates_skipped": 0,
            "small_images_skipped": 0,
            "errors": []
        }

        print(f"PDFå…±æœ‰ {len(doc)} é¡µ")

        for page_num in range(len(doc)):
            print(f"å¤„ç†ç¬¬ {page_num + 1}/{len(doc)} é¡µ...")

            try:
                page = doc.load_page(page_num)
                image_list = page.get_images(full=True)

                extraction_stats["total_images_found"] += len(image_list)

                for img_index, img in enumerate(image_list):
                    try:
                        xref = img[0]

                        # è·å–å›¾ç‰‡æ•°æ®
                        img_dict = doc.extract_image(xref)
                        img_data = img_dict["image"]
                        img_ext = img_dict["ext"]

                        # æ£€æŸ¥å›¾ç‰‡å¤§å°
                        if len(img_data) < min_size:
                            extraction_stats["small_images_skipped"] += 1
                            continue

                        # æ£€æŸ¥é‡å¤
                        img_hash = self.get_image_hash(img_data)
                        if skip_duplicates and img_hash in self.duplicate_hashes:
                            extraction_stats["duplicates_skipped"] += 1
                            print(
                                f"  â© è·³è¿‡é‡å¤å›¾ç‰‡: {img_hash[:8]}... (ç¬¬{page_num+1}é¡µ)")
                            continue

                        self.duplicate_hashes.add(img_hash)
                        print(f"  âœ“ æ–°å›¾ç‰‡å“ˆå¸Œ: {img_hash[:8]}... (ç¬¬{page_num+1}é¡µ)")

                        # è·å–å›¾ç‰‡ä½ç½®ä¿¡æ¯
                        img_rect = None
                        try:
                            img_rects = page.get_image_rects(xref)
                            if img_rects:
                                img_rect = img_rects[0]
                        except:
                            pass

                        # ç”Ÿæˆæ–‡ä»¶åå€™é€‰
                        candidates = self.get_image_name_candidates(
                            doc, xref, page_num, img_index, img_rect, page
                        )

                        # é€‰æ‹©æœ€ç»ˆæ–‡ä»¶å
                        filename = self.generate_final_filename(
                            candidates, img_ext, page_num, img_index, img_hash
                        )

                        # ä¿å­˜å›¾ç‰‡
                        output_path = self.output_dir / filename
                        with open(output_path, "wb") as f:
                            f.write(img_data)

                        # è®°å½•æå–ä¿¡æ¯
                        extract_info = {
                            "filename": filename,
                            "page": page_num + 1,
                            "index": img_index,
                            "size_bytes": len(img_data),
                            "format": img_ext,
                            "hash": img_hash,
                            "candidates_tried": candidates,
                            "dimensions": f"{img_dict.get('width', 'unknown')}x{img_dict.get('height', 'unknown')}"
                        }

                        if img_rect:
                            extract_info["position"] = {
                                "x": round(img_rect.x0, 2),
                                "y": round(img_rect.y0, 2),
                                "width": round(img_rect.width, 2),
                                "height": round(img_rect.height, 2)
                            }

                        self.extraction_log.append(extract_info)
                        extraction_stats["images_extracted"] += 1

                        print(f"  âœ“ æå–: {filename} ({len(img_data)} bytes)")

                    except Exception as e:
                        error_msg = f"ç¬¬{page_num+1}é¡µå›¾ç‰‡{img_index}æå–å¤±è´¥: {e}"
                        extraction_stats["errors"].append(error_msg)
                        print(f"  âœ— {error_msg}")

            except Exception as e:
                error_msg = f"å¤„ç†ç¬¬{page_num+1}é¡µæ—¶å‡ºé”™: {e}"
                extraction_stats["errors"].append(error_msg)
                print(f"âœ— {error_msg}")

        doc.close()
        return extraction_stats

    def save_extraction_report(self, pdf_path: str, stats: Dict):
        """ä¿å­˜æå–æŠ¥å‘Š"""
        report = {
            "extraction_time": datetime.now().isoformat(),
            "source_pdf": os.path.basename(pdf_path),
            "output_directory": str(self.output_dir),
            "statistics": stats,
            "extracted_images": self.extraction_log
        }

        report_path = self.output_dir / "extraction_report.json"
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“Š æå–æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        # æ‰“å°æ‘˜è¦
        print(f"""
ğŸ“ˆ æå–å®Œæˆï¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“„ æºæ–‡ä»¶: {os.path.basename(pdf_path)}
ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}
ğŸ” å‘ç°å›¾ç‰‡: {stats['total_images_found']} ä¸ª
âœ… æˆåŠŸæå–: {stats['images_extracted']} ä¸ª
ğŸ”„ è·³è¿‡é‡å¤: {stats['duplicates_skipped']} ä¸ª
ğŸ“ è·³è¿‡å°å›¾: {stats['small_images_skipped']} ä¸ª
âŒ æå–å¤±è´¥: {len(stats['errors'])} ä¸ª
""")


def main():
    parser = argparse.ArgumentParser(description="PDFå›¾ç‰‡æå–Pipeline")
    parser.add_argument("pdf_path", help="PDFæ–‡ä»¶è·¯å¾„")
    parser.add_argument("-o", "--output", default="extracted_images",
                        help="è¾“å‡ºç›®å½• (é»˜è®¤: extracted_images)")
    parser.add_argument("--no-dedup", action="store_true",
                        help="ä¸è·³è¿‡é‡å¤å›¾ç‰‡")
    parser.add_argument("--min-size", type=int, default=1000,
                        help="æœ€å°å›¾ç‰‡å¤§å°(å­—èŠ‚) (é»˜è®¤: 1000)")

    args = parser.parse_args()

    if not os.path.exists(args.pdf_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.pdf_path}")
        return

    try:
        extractor = PDFImageExtractor(args.output)
        stats = extractor.extract_images_from_pdf(
            args.pdf_path,
            skip_duplicates=not args.no_dedup,
            min_size=args.min_size
        )
        extractor.save_extraction_report(args.pdf_path, stats)

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")


if __name__ == "__main__":
    main()


# ä½¿ç”¨ç¤ºä¾‹ï¼š
"""
# åŸºæœ¬ä½¿ç”¨
python pdf_extractor.py document.pdf

# æŒ‡å®šè¾“å‡ºç›®å½•
python pdf_extractor.py document.pdf -o my_images

# ä¸è·³è¿‡é‡å¤å›¾ç‰‡
python pdf_extractor.py document.pdf --no-dedup

# è®¾ç½®æœ€å°å›¾ç‰‡å¤§å°
python pdf_extractor.py document.pdf --min-size 5000

# åœ¨ä»£ç ä¸­ä½¿ç”¨
extractor = PDFImageExtractor("output_folder")
stats = extractor.extract_images_from_pdf("example.pdf")
extractor.save_extraction_report("example.pdf", stats)
"""
